import asyncio
import json
import time
import threading
import pandas as pd
import math
from contextlib import redirect_stdout, redirect_stderr
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from dataclasses import dataclass, field
from typing import Dict, List, Any, Literal
from pathlib import Path
from tabulate import tabulate
from functools import partial
from enum import Enum

from vlmeval.smp import load, dump, get_logger
from vlmeval.smp.log import setup_subprocess_logger

logger = get_logger(__name__)

FAIL_MSG = 'Failed to obtain answer via API.'
DatasetType = Literal["image", "video", "mt"]


def _eval_subprocess_target(
    dataset_obj,
    result_file: str,
    judge_kwargs: dict,
    log_file: str,
):
    """Evaluate function in child processes."""
    setup_subprocess_logger(log_file)
    logger.info(f"ğŸ”” [Eval Start] {dataset_obj.dataset_name}")

    try:
        # è°ƒç”¨ evaluate æ–¹æ³•
        with open(log_file, 'a') as f:
            with redirect_stdout(f), redirect_stderr(f):
                result = dataset_obj.evaluate(result_file, **judge_kwargs)

        # åºåˆ—åŒ–ç»“æœï¼ˆDataFrame è½¬ dictï¼‰
        if isinstance(result, pd.DataFrame):
            result = result.to_dict()

        return {'success': True, 'result': result, 'error': None}

    except Exception as e:
        import traceback
        error_msg = f"{type(e).__name__}: {e}\n{traceback.format_exc()}"
        print(f"\nâŒ EVALUATION ERROR: {error_msg}")

        return {'success': False, 'result': None, 'error': error_msg}


def chat_mt(model, messages: List[dict], dataset_name: str) -> List[str]:
    """Infer function for multi-turn dataset."""
    assert len(messages) % 2 == 0, "Messages should be pairs of user and assistant"
    nturn = len(messages) // 2
    utter_stack = []
    predictions = []

    for i in range(nturn):
        utter = messages[2 * i]
        utter_stack.append(utter)
        try:
            resp = model.chat(utter_stack, dataset=dataset_name)
            utter_stack.append(dict(role='assistant', content=resp))
        except Exception as e:
            resp = FAIL_MSG + str(e)
            utter_stack.append(dict(role='assistant', content=resp))
        predictions.append(resp)

    return predictions

# ==========================================
# Core Data Structures
# ==========================================

class EvalStatus(Enum):
    Pending = 1
    Running = 2
    Done = 3
    Error = 4
    Skipped = 5


@dataclass
class InferenceTask:
    """å•ä¸ªæ¨ç†ä»»åŠ¡"""
    dataset_name: str      # æ•°æ®é›†åç§°ï¼ˆç”¨äºå‘½åï¼‰
    model_name: str        # æ¨¡å‹åç§°ï¼ˆç”¨äºå‘½åï¼‰
    sample_index: str      # æ ·æœ¬ID
    prompt_struct: Any     # å·²æ„å»ºçš„promptç»“æ„
    dataset_type: DatasetType = "image"  # æ•°æ®é›†ç±»å‹

@dataclass
class DatasetConfig:
    """æ•°æ®é›†é…ç½®å’ŒçŠ¶æ€"""
    dataset_name: str      # æ•°æ®é›†åç§°
    dataset_obj: Any       # æ•°æ®é›†å¯¹è±¡
    model_obj: Any         # æ¨¡å‹å¯¹è±¡
    model_name: str        # æ¨¡å‹åç§°
    work_dir: str          # å·¥ä½œç›®å½•
    result_file: str       # ç»“æœæ–‡ä»¶è·¯å¾„
    judge_kwargs: dict     # judgeå‚æ•°
    verbose: bool = False  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    # æ•°æ®é›†ç±»å‹: "image", "video", "mt"
    dataset_type: DatasetType = "image"

    # Video ç‰¹æœ‰é…ç½®
    video_llm: bool = False  # æ˜¯å¦ä½¿ç”¨è§†é¢‘æ¨¡å¼ï¼ˆvs å¤šå›¾æ¨¡å¼ï¼‰

    # Runtime state
    total_samples: int = 0
    processed: int = 0
    eval_status: EvalStatus = EvalStatus.Pending
    eval_start_time: float = 0.0
    eval_duration: float = 0.0

    # Inference timing statistics
    infer_start_time: float = 0.     # æ¨ç†å¼€å§‹æ—¶é—´(ç§’)
    infer_end_time: float = 0.       # æ¨ç†ç»“æŸæ—¶é—´(ç§’)
    infer_total_time: float = 0.     # æ¨ç†ç´¯è®¡è€—æ—¶(ç§’)
    infer_count: int = 0             # æ¨ç†æ ·æœ¬æ•°

    # For final result assembly
    results_dict: Dict[str, Any] = field(default_factory=dict)

# ==========================================
# Main Pipeline Class
# ==========================================

class APIEvalPipeline:
    """
    APIæ¨¡å‹æ¨ç†å’Œè¯„æµ‹æµæ°´çº¿

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. è·¨æ•°æ®é›†çš„ç»Ÿä¸€æ¨ç†é˜Ÿåˆ—
    2. æ¨ç†å’Œè¯„æµ‹å¹¶è¡Œæ‰§è¡Œ
    3. æ–­ç‚¹ç»­ä¼ 
    4. å®æ—¶çŠ¶æ€ç›‘æ§
    5. çº¿ç¨‹å®‰å…¨çš„ç»“æœä¿å­˜
    """

    def __init__(
        self,
        dataset_configs: List[DatasetConfig],
        concurrency: int = 32,
        monitor_interval: int = 30,
        run_infer: bool = True,
        run_eval: bool = True,
        debug: bool = False,
    ):
        """
        Args:
            dataset_configs: æ•°æ®é›†é…ç½®åˆ—è¡¨
            concurrency: æ¨ç†å¹¶å‘æ•°
            monitor_interval: çŠ¶æ€ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
            run_infer: æ˜¯å¦è¿è¡Œæ¨ç†
            run_eval: æ˜¯å¦è¿è¡Œè¯„æµ‹
            debug: è°ƒè¯•æ¨¡å¼ï¼Œåœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œè¯„æµ‹ï¼ˆæ”¯æŒ ipdb æ–­ç‚¹ï¼‰
        """
        self.dataset_configs = dataset_configs
        self.concurrency = concurrency
        self.monitor_interval = monitor_interval
        self.run_infer = run_infer
        self.run_eval = run_eval
        self.infer_executor = ThreadPoolExecutor(max_workers=concurrency)
        self.eval_executor = ProcessPoolExecutor(max_workers=4)
        self.producer_executor = ProcessPoolExecutor(max_workers=1)
        self.debug = debug

        # æ ¸å¿ƒç»„ä»¶
        self.queue = asyncio.Queue(maxsize=concurrency * 2)
        self.states: Dict[str, DatasetConfig] = {
            cfg.dataset_name: cfg for cfg in dataset_configs
        }
        self.file_locks: Dict[str, threading.Lock] = {
            cfg.dataset_name: threading.Lock() for cfg in dataset_configs
        }

        # è¿è¡Œæ—¶çŠ¶æ€
        self.start_time = 0.0
        self.active_workers = 0
        self.total_tasks_generated = 0

        # åˆ›å»ºå·¥ä½œç›®å½•
        for cfg in dataset_configs:
            Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
            if not self.run_eval:
                cfg.eval_status = EvalStatus.Skipped

    def _get_checkpoint_file(self, dataset_name: str) -> Path:
        """è·å–æ–­ç‚¹æ–‡ä»¶è·¯å¾„ï¼ˆæ¨ç†ä¸­é—´ç»“æœï¼‰"""
        cfg = self.states[dataset_name]
        return Path(cfg.work_dir) / f"{cfg.model_name}_{dataset_name}_checkpoint.pkl"

    def _load_checkpoint(self, dataset_name: str) -> Dict[str, Any]:
        """
        åŠ è½½æ–­ç‚¹ï¼šè¯»å–å·²å®Œæˆçš„æ¨ç†ç»“æœ

        ä¼˜å…ˆçº§ï¼š
        1. checkpoint.pklï¼ˆæ¨ç†ä¸­é—´ç»“æœï¼‰
        2. result_fileï¼ˆæœ€ç»ˆç»“æœæ–‡ä»¶ï¼‰
        """
        cfg = self.states[dataset_name]
        results = {}

        # 1. å°è¯•åŠ è½½checkpoint
        checkpoint_file = self._get_checkpoint_file(dataset_name)
        if checkpoint_file.exists():
            try:
                results = load(str(checkpoint_file))
                logger.info(f"   [{dataset_name}] Loaded {len(results)} results from checkpoint")
            except Exception as e:
                logger.warning(f"   [{dataset_name}] Failed to load checkpoint: {e}")

        # 2. å°è¯•ä»æœ€ç»ˆç»“æœæ–‡ä»¶åŠ è½½
        result_path = Path(cfg.result_file)
        if result_path.exists():
            try:
                data = load(str(result_path))
                if isinstance(data, pd.DataFrame):
                    # DataFrameæ ¼å¼ï¼šæå–indexå’Œpredictionåˆ—
                    existing_results = {
                        str(idx): pred
                        for idx, pred in zip(data['index'], data['prediction'])
                        if FAIL_MSG not in str(pred)
                    }
                    results.update(existing_results)
                    logger.info(f"   [{dataset_name}] Loaded {len(existing_results)} results from result file")
            except Exception as e:
                logger.warning(f"   [{dataset_name}] Failed to load result file: {e}")

        return results

    def _save_checkpoint(self, dataset_name: str, result: dict):
        """çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜å•ä¸ªç»“æœåˆ°checkpoint"""
        cfg = self.states[dataset_name]
        checkpoint_file = self._get_checkpoint_file(dataset_name)

        with self.file_locks[dataset_name]:
            # åŠ è½½ç°æœ‰ç»“æœ
            if checkpoint_file.exists():
                results = load(str(checkpoint_file))
            else:
                results = {}

            # æ›´æ–°ç»“æœ
            results[result['index']] = result['prediction']

            # ä¿å­˜
            dump(results, str(checkpoint_file))

    def _save_final_result(self, dataset_name: str):
        """
        ä¿å­˜æœ€ç»ˆç»“æœæ–‡ä»¶

        å°†æ‰€æœ‰æ¨ç†ç»“æœæ•´åˆä¸ºDataFrameæ ¼å¼ï¼Œä¿å­˜ä¸ºresult_file
        """
        cfg = self.states[dataset_name]

        with self.file_locks[dataset_name]:
            # ä»æ•°æ®é›†å¯¹è±¡è·å–å®Œæ•´æ•°æ®
            dataset_data = cfg.dataset_obj.data.copy()

            # ç¡®ä¿æ‰€æœ‰æ ·æœ¬éƒ½æœ‰ç»“æœ
            missing_indices = []
            for idx in dataset_data['index']:
                idx_str = str(idx)
                if idx_str not in cfg.results_dict:
                    missing_indices.append(idx_str)

            if missing_indices:
                logger.warning(
                    f"   [{dataset_name}] Missing results for {len(missing_indices)} samples: "
                    f"{missing_indices[:5]}{'...' if len(missing_indices) > 5 else ''}"
                )
                return False

            # æ„å»ºpredictionåˆ—
            predictions = [cfg.results_dict[str(idx)] for idx in dataset_data['index']]
            dataset_data['prediction'] = predictions

            # ç§»é™¤imageåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'image' in dataset_data:
                dataset_data.pop('image')

            # ä¿å­˜æœ€ç»ˆç»“æœ
            dump(dataset_data, cfg.result_file)
            logger.info(f"   [{dataset_name}] Saved final results to {cfg.result_file}")

            # åˆ é™¤checkpointæ–‡ä»¶
            checkpoint_file = self._get_checkpoint_file(dataset_name)
            if checkpoint_file.exists():
                checkpoint_file.unlink()

            return True

    async def _producer(self):
        """
        ä»»åŠ¡ç”Ÿæˆå™¨ï¼šæ‰«ææ‰€æœ‰æ•°æ®é›†ï¼Œæ„å»ºpromptï¼Œå¡«å……é˜Ÿåˆ—

        æ”¯æŒä¸‰ç§æ•°æ®é›†ç±»å‹ï¼š
        - image: åŸºç¡€å›¾åƒæ•°æ®é›†
        - video: è§†é¢‘æ•°æ®é›†ï¼ˆæ”¯æŒ pack æ¨¡å¼ï¼‰
        - mt: å¤šè½®å¯¹è¯æ•°æ®é›†
        """
        logger.info("ğŸ“¦ Initializing tasks and checking checkpoints...")

        for cfg in self.dataset_configs:
            dataset_name = cfg.dataset_name
            dataset_type = cfg.dataset_type

            logger.info(
                f"   [{dataset_name}] Start build prompt [type={dataset_type}]"
            )
            # 1. åŠ è½½æ–­ç‚¹
            existing_results = self._load_checkpoint(dataset_name)
            cfg.results_dict = existing_results
            cfg.processed = len(existing_results)
            cfg.total_samples = len(cfg.dataset_obj.data)

            # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ¨ç†
            if (not self.run_infer) or cfg.processed == cfg.total_samples:
                logger.info(
                    f"   [{dataset_name}] All samples completed ({cfg.processed}/{cfg.total_samples}). "
                    "Will trigger eval directly."
                )
                # å¦‚æœæ²¡æœ‰æœ€ç»ˆç»“æœæ–‡ä»¶ï¼Œéœ€è¦å…ˆä¿å­˜
                if not Path(cfg.result_file).exists():
                    self._save_final_result(dataset_name)
                # è§¦å‘è¯„æµ‹
                asyncio.create_task(self._trigger_eval(dataset_name))
                continue

            # 3. æ ¹æ®æ•°æ®é›†ç±»å‹å¤„ç†
            if dataset_type == "video":
                tasks_generated = await self._produce_video_tasks(cfg, existing_results)
            elif dataset_type == "mt":
                tasks_generated = await self._produce_mt_tasks(cfg, existing_results)
            else:  # image
                tasks_generated = await self._produce_image_tasks(cfg, existing_results)

            # å¦‚æœæ²¡èƒ½æˆåŠŸæ„é€  promptï¼Œè¯´æ˜æ•°æ®é›†åŠ è½½æœ‰é—®é¢˜ï¼Œè·³è¿‡è¯„æµ‹
            if tasks_generated == 0:
                cfg.eval_status = EvalStatus.Skipped

            self.total_tasks_generated += tasks_generated
            logger.info(
                f"   [{dataset_name}] Generated {tasks_generated} tasks "
                f"(Skipped {cfg.processed}) [type={dataset_type}]"
            )

        # æ”¾å…¥ç»“æŸæ ‡è®°
        for _ in range(self.concurrency):
            await self.queue.put(None)

        logger.info(f"ğŸš€ Queue ready. Total pending tasks: {self.total_tasks_generated}")

    async def _produce_image_tasks(self, cfg: DatasetConfig, existing_results: dict) -> int:
        """ç”Ÿæˆå›¾åƒæ•°æ®é›†çš„æ¨ç†ä»»åŠ¡"""
        model = cfg.model_obj
        dataset = cfg.dataset_obj
        dataset_name = cfg.dataset_name
        data = dataset.data

        if hasattr(model, 'set_dump_image'):
            model.set_dump_image(dataset.dump_image)

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰prompt
        use_custom_prompt = (
            hasattr(model, 'use_custom_prompt')
            and model.use_custom_prompt(dataset_name)
        )
        if use_custom_prompt:
            logger.info(f"   [{dataset_name}] Using model custom prompt")
        else:
            logger.info(f"   [{dataset_name}] Using vanilla dataset prompt")

        tasks_generated = 0
        for i in range(len(data)):
            item = data.iloc[i]
            idx_str = str(item['index'])

            if idx_str in existing_results:
                continue

            try:
                if use_custom_prompt:
                    prompt_struct = model.build_prompt(item, dataset=dataset_name)
                else:
                    prompt_struct = dataset.build_prompt(item)
            except Exception as e:
                logger.error(f"   [{dataset_name}] Failed to build prompt for sample {idx_str}: {e}")
                continue

            task = InferenceTask(
                dataset_name=dataset_name,
                model_name=cfg.model_name,
                sample_index=idx_str,
                prompt_struct=prompt_struct,
                dataset_type="image"
            )
            await self.queue.put(task)
            tasks_generated += 1

        return tasks_generated

    async def _produce_video_tasks(self, cfg: DatasetConfig, existing_results: dict) -> int:
        """
        ç”Ÿæˆè§†é¢‘æ•°æ®é›†çš„æ¨ç†ä»»åŠ¡

        è§†é¢‘æ•°æ®é›†çš„ç‰¹æ®Šå¤„ç†ï¼š
        1. æ”¯æŒ video_llm æ¨¡å¼ï¼ˆåŸç”Ÿè§†é¢‘è¾“å…¥ï¼‰å’Œå¤šå›¾æ¨¡å¼
        2. ä½¿ç”¨ dataset.build_prompt(sample, video_llm=...) æ„å»º prompt
        """
        loop = asyncio.get_running_loop()
        model = cfg.model_obj
        dataset = cfg.dataset_obj
        dataset_name = cfg.dataset_name
        data = dataset.data

        # è·å– video_llm è®¾ç½®
        video_llm = cfg.video_llm

        logger.info(f"   [{dataset_name}] Video mode: video_llm={video_llm}")

        tasks_generated = 0
        for i in range(len(data)):
            item = data.iloc[i]
            idx_str = str(item['index'])

            if idx_str in existing_results:
                continue

            try:
                # å•ç‹¬äº¤ç»™ä¸€ä¸ª CPU å¤„ç†ï¼Œä»¥é¿å… video å¤„ç†çš„é«˜ CPU å ç”¨å½±å“ä¸»è¿›ç¨‹
                prompt_struct = await loop.run_in_executor(
                    self.producer_executor,
                    partial(dataset.build_prompt, i, video_llm=video_llm),
                )
                if prompt_struct is None:
                    continue
            except Exception as e:
                logger.error(f"   [{dataset_name}] Failed to build prompt for sample {idx_str}: {repr(e)}")
                continue

            task = InferenceTask(
                dataset_name=dataset_name,
                model_name=cfg.model_name,
                sample_index=idx_str,
                prompt_struct=prompt_struct,
                dataset_type="video"
            )
            await self.queue.put(task)
            tasks_generated += 1

        return tasks_generated

    async def _produce_mt_tasks(self, cfg: DatasetConfig, existing_results: dict) -> int:
        """
        ç”Ÿæˆå¤šè½®å¯¹è¯æ•°æ®é›†çš„æ¨ç†ä»»åŠ¡

        å¤šè½®å¯¹è¯æ•°æ®é›†çš„ç‰¹æ®Šå¤„ç†ï¼š
        1. prompt_struct æ˜¯ messages åˆ—è¡¨
        2. æ¨ç†æ—¶ä½¿ç”¨ chat_mt å‡½æ•°è¿›è¡Œå¤šè½®å¯¹è¯
        """
        model = cfg.model_obj
        dataset = cfg.dataset_obj
        dataset_name = cfg.dataset_name
        data = dataset.data

        # éªŒè¯æ¨¡å‹æ”¯æŒå¤šè½®å¯¹è¯
        if not hasattr(model, 'chat_inner'):
            logger.warning(f"   [{dataset_name}] Model does not support multi-turn chat (no chat_inner method)")

        logger.info(f"   [{dataset_name}] Multi-turn dialogue mode")

        tasks_generated = 0
        for i in range(len(data)):
            item = data.iloc[i]
            idx_str = str(item['index'])

            if idx_str in existing_results:
                continue

            try:
                # å¤šè½®å¯¹è¯çš„ prompt æ˜¯ messages åˆ—è¡¨
                prompt_struct = dataset.build_prompt(item)
            except Exception as e:
                logger.error(f"   [{dataset_name}] Failed to build prompt for sample {idx_str}: {e}")
                continue

            task = InferenceTask(
                dataset_name=dataset_name,
                model_name=cfg.model_name,
                sample_index=idx_str,
                prompt_struct=prompt_struct,
                dataset_type="mt"
            )
            await self.queue.put(task)
            tasks_generated += 1

        return tasks_generated

    async def _worker(self):
        """
        æ¨ç†å·¥ä½œçº¿ç¨‹

        æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ä¸åŒçš„æ¨ç†æ–¹æ³•ï¼š
        - image/video: model.generate()
        - mt: chat_mt() å¤šè½®å¯¹è¯
        """
        loop = asyncio.get_running_loop()

        while True:
            task = await self.queue.get()
            if task is None:
                self.queue.task_done()
                break

            self.active_workers += 1
            cfg = self.states[task.dataset_name]
            try:
                model = cfg.model_obj
                start_time = time.time()
                if not cfg.infer_start_time:
                    cfg.infer_start_time = start_time

                # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©æ¨ç†æ–¹å¼
                if task.dataset_type == "mt":
                    # å¤šè½®å¯¹è¯æ¨ç†
                    def mt_inference_call():
                        return chat_mt(model, task.prompt_struct, task.dataset_name)

                    output = await loop.run_in_executor(self.infer_executor, mt_inference_call)
                else:
                    # å›¾åƒ/è§†é¢‘æ¨ç†ï¼ˆä½¿ç”¨ generateï¼‰
                    def inference_call():
                        return model.generate(
                            message=task.prompt_struct,
                            dataset=task.dataset_name
                        )

                    output = await loop.run_in_executor(self.infer_executor, inference_call)

                inference_time = time.time() - start_time

                # ä¿å­˜ç»“æœ
                result_item = {
                    "index": task.sample_index,
                    "prediction": output
                }
                self._save_checkpoint(task.dataset_name, result_item)

                # æ›´æ–°çŠ¶æ€
                cfg.results_dict[task.sample_index] = output
                cfg.infer_end_time = time.time()
                cfg.infer_total_time += inference_time
                cfg.infer_count += 1
                cfg.processed += 1

                # æ‰“å°è¯¦ç»†ä¿¡æ¯
                if cfg.verbose:
                    output_preview = str(output)[:100] if output else ""
                    logger.info(
                        f"[{task.dataset_name}] Sample {task.sample_index}: "
                        f"{output_preview}... (took {inference_time:.2f}s)"
                    )

                # æ£€æŸ¥æ˜¯å¦è§¦å‘è¯„æµ‹
                if cfg.processed == cfg.total_samples and cfg.eval_status == EvalStatus.Pending:
                    if self._save_final_result(task.dataset_name):
                        asyncio.create_task(self._trigger_eval(task.dataset_name))

            except Exception as e:
                logger.error(
                    f"âŒ Worker error on {task.dataset_name}/{task.sample_index}: {e}",
                    exc_info=True
                )
            finally:
                self.active_workers -= 1
                self.queue.task_done()

    async def _trigger_eval(self, dataset_name: str):
        """
        è§¦å‘è¯„æµ‹ä»»åŠ¡

        æ­£å¸¸æ¨¡å¼ï¼šåœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œ
        è°ƒè¯•æ¨¡å¼ï¼šåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œ
        """
        cfg = self.states[dataset_name]

        # é˜²æ­¢é‡å¤è§¦å‘
        if cfg.eval_status != EvalStatus.Pending:
            return

        cfg.eval_status = EvalStatus.Running
        cfg.eval_start_time = time.time()

        # è¯„æµ‹æ—¥å¿—æ–‡ä»¶
        eval_log_dir = Path(cfg.work_dir) / 'eval_logs'
        eval_log_dir.mkdir(parents=True, exist_ok=True)
        eval_log_path = str(eval_log_dir / f"{cfg.model_name}_{dataset_name}_eval.log")

        if self.debug:
            # è°ƒè¯•æ¨¡å¼ï¼šç›´æ¥åœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œè¯„æµ‹
            await self._run_eval_in_main_process(dataset_name, eval_log_path)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨å­è¿›ç¨‹è¿è¡Œè¯„æµ‹
            await self._run_eval_in_subprocess(dataset_name, eval_log_path)

    async def _run_eval_in_main_process(self, dataset_name: str, eval_log_path: str):
        """åœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œè¯„æµ‹"""
        cfg = self.states[dataset_name]
        logger.info(f"ğŸ”” [Eval Start - Debug Mode] {dataset_name}")

        def run_eval():
            try:
                result = cfg.dataset_obj.evaluate(cfg.result_file, **cfg.judge_kwargs)

                # åºåˆ—åŒ–ç»“æœï¼ˆDataFrame è½¬ dictï¼‰
                if isinstance(result, pd.DataFrame):
                    result = result.to_dict()

                return {'success': True, 'result': result, 'error': None}

            except Exception as e:
                import traceback
                error_msg = f"{type(e).__name__}: {e}\n{traceback.format_exc()}"
                return {'success': False, 'result': None, 'error': error_msg}

        try:
            eval_result = run_eval()
            self._handle_eval_result(dataset_name, eval_result, eval_log_path)

        except Exception as e:
            cfg.eval_status = EvalStatus.Error
            cfg.eval_duration = time.time() - cfg.eval_start_time
            logger.error(f"âŒ [Eval Failed] {dataset_name}: {e}")

    async def _run_eval_in_subprocess(self, dataset_name: str, eval_log_path: str):
        """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œè¯„æµ‹"""
        cfg = self.states[dataset_name]

        # åœ¨åå°çº¿ç¨‹ä¸­ç­‰å¾…å­è¿›ç¨‹å®Œæˆï¼ˆé¿å…é˜»å¡ asyncio äº‹ä»¶å¾ªç¯ï¼‰
        loop = asyncio.get_running_loop()

        try:
            eval_result = await loop.run_in_executor(
                self.eval_executor,
                _eval_subprocess_target,
                cfg.dataset_obj,
                cfg.result_file,
                cfg.judge_kwargs,
                eval_log_path,
            )
            self._handle_eval_result(dataset_name, eval_result, eval_log_path)

        except Exception as e:
            cfg.eval_status = EvalStatus.Error
            cfg.eval_duration = time.time() - cfg.eval_start_time
            logger.error(
                f"âŒ [Eval Failed] {dataset_name}: {e}\n"
                f"   Check log file: {eval_log_path}"
            )

    def _handle_eval_result(self, dataset_name: str, eval_result: dict, eval_log_path: str):
        """å¤„ç†è¯„æµ‹ç»“æœ"""
        cfg = self.states[dataset_name]

        if eval_result['success']:
            cfg.eval_status = EvalStatus.Done
            cfg.eval_duration = time.time() - cfg.eval_start_time
            logger.info(
                f"âœ… [Eval Finish] {dataset_name} (Took {cfg.eval_duration:.1f}s)"
            )

            # æ‰“å°è¯„æµ‹ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            result = eval_result['result']
            if result is not None:
                if isinstance(result, dict):
                    # å¯èƒ½æ˜¯ DataFrame.to_dict() çš„ç»“æœï¼Œå°è¯•è¿˜åŸ
                    try:
                        df = pd.DataFrame(result)
                        logger.info(f"   Results:\n{df}")
                    except Exception:
                        logger.info(f"   Results: {json.dumps(result, indent=2)}")
        else:
            cfg.eval_status = EvalStatus.Error
            cfg.eval_duration = time.time() - cfg.eval_start_time
            logger.error(
                f"âŒ [Eval Failed] {dataset_name}: {eval_result['error']}\n"
                f"   Check log file: {eval_log_path}"
            )

    async def _monitor_loop(self):
        """åå°ç›‘æ§ï¼šå‘¨æœŸæ€§æ‰“å°çŠ¶æ€å¿«ç…§"""
        self._log_snapshot()
        while True:
            await asyncio.sleep(self.monitor_interval)
            self._log_snapshot()

            # é€€å‡ºæ¡ä»¶ï¼šæ‰€æœ‰æ¨ç†å®Œæˆ ä¸” æ‰€æœ‰è¯„æµ‹å®Œæˆ
            all_infer_done = (
                self.queue.empty()
                and self.active_workers == 0
            )
            all_eval_done = all(
                cfg.eval_status in [EvalStatus.Done, EvalStatus.Error, EvalStatus.Skipped]
                for cfg in self.states.values()
            )

            if all_infer_done and all_eval_done:
                break

    def _log_snapshot(self):
        """ç”Ÿæˆæ¸…æ™°çš„çŠ¶æ€æ—¥å¿—"""
        elapsed = time.time() - self.start_time
        lines = []

        total_processed = 0
        total_samples = 0

        for cfg in self.states.values():
            total_processed += cfg.processed
            total_samples += cfg.total_samples

            # æ¨ç†è¿›åº¦
            if cfg.total_samples > 0:
                infer_pct = cfg.processed / cfg.total_samples
                # ä¿ç•™ä¸€ä½å°æ•°ï¼Œå‘ä¸‹å–æ•´ï¼Œ100% ä»£è¡¨å…¨éƒ¨å®Œæˆ
                infer_pct = math.floor(infer_pct * 1000) / 10
            else:
                infer_pct = 0
            infer_str = f"{infer_pct:>5.1f}% ({cfg.processed:>4}/{cfg.total_samples:<4})"

            # å¹³å‡æ¨ç†è€—æ—¶
            if cfg.infer_count > 0:
                if cfg.processed == cfg.total_samples:
                    avg_time = (cfg.infer_end_time - cfg.infer_start_time) / cfg.infer_count
                else:
                    avg_time = (time.time() - cfg.infer_start_time) / cfg.infer_count
                if avg_time > 1:
                    infer_str += f" | {avg_time:.2f}s/it"
                elif avg_time > 0:
                    infer_str += f" | {1 / avg_time:.2f}it/s"

            # è¯„æµ‹çŠ¶æ€
            eval_str = cfg.eval_status.name
            if cfg.eval_status == EvalStatus.Running:
                dur = time.time() - cfg.eval_start_time
                eval_str = f"Running ({dur:.0f}s)"
            elif cfg.eval_status == EvalStatus.Done:
                eval_str = f"Done ({cfg.eval_duration:.1f}s)"

            # æ ¼å¼åŒ–è¾“å‡º
            name_str = cfg.dataset_name[:20].ljust(20)
            line = [name_str, f"Infer: {infer_str} | Eval: {eval_str:<20}"]
            lines.append(line)

        # å…¨å±€è¿›åº¦
        global_pct = (total_processed / total_samples * 100) if total_samples > 0 else 0
        global_str = f"Infer: {global_pct:>5.1f}% ({total_processed:>4}/{total_samples:<4})"

        logger.info('\n' + tabulate(lines, [f"Elapsed: {elapsed:.0f}s", global_str], tablefmt='simple_outline'))

    async def run(self):
        """å¯åŠ¨æµæ°´çº¿"""
        self.start_time = time.time()

        logger.info("  API Inference & Evaluation Pipeline")
        logger.info(f"  Datasets: {len(self.dataset_configs)}")
        logger.info(f"  Concurrency: {self.concurrency}")
        logger.info(f"  Infer: {self.run_infer}")
        logger.info(f"  Eval: {self.run_eval}")

        # 1. å¯åŠ¨ç”Ÿäº§è€…ï¼ˆä¸ç­‰å¾…å®Œæˆï¼Œè®©å…¶ä¸æ¶ˆè´¹è€…å¹¶è¡Œï¼‰
        producer_task = asyncio.create_task(self._producer())

        # 2. å¯åŠ¨ç›‘æ§
        monitor_task = asyncio.create_task(self._monitor_loop())

        # 3. å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆä¸ç”Ÿäº§è€…å¹¶è¡Œè¿è¡Œï¼‰
        if self.run_infer:
            workers = [
                asyncio.create_task(self._worker())
                for _ in range(self.concurrency)
            ]
            # ç­‰å¾…ç”Ÿäº§è€…å®Œæˆï¼ˆé˜Ÿåˆ—å¡«å……ç»“æŸæ ‡è®°ï¼‰
            await producer_task
            # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…å®Œæˆ
            await asyncio.gather(*workers)
            logger.info("ğŸ‰ All inference tasks finished. Waiting for pending evaluations...")
        else:
            await producer_task
            logger.info("ğŸ“Š Eval mode only. Skipping inference...")

        # 4. ç­‰å¾…ç›‘æ§å¾ªç¯ç»“æŸï¼ˆæ‰€æœ‰è¯„æµ‹å®Œæˆï¼‰
        await monitor_task

        # 5. æœ€ç»ˆæŠ¥å‘Š
        self._log_snapshot()
        total_time = time.time() - self.start_time
        logger.info(f"  ğŸ Pipeline Completed in {total_time:.1f}s")
