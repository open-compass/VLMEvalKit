# -*- coding: utf-8 -*-
import os
import re
import json
import pandas as pd
import numpy as np
import warnings
import time
import threading
import datetime
from functools import partial
import multiprocessing as mp

from .image_base import ImageBaseDataset
from .utils import build_judge, DEBUG_MESSAGE
from .utils.hipho_verifier import grade, extract_boxed_answer, get_answer_str, answer_tag_reward_fn_for_r1
from .utils.prompt_inference import SYSTEM_PROMPTS_EN, SYSTEM_PROMPTS_ZH
from ..smp import *
from ..smp.file import get_intermediate_file_path
from ..utils import track_progress_rich

# çº¿ç¨‹é”ç”¨äºåŒæ­¥è¾“å‡º
output_lock = threading.Lock()

def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with output_lock:
        print(*args, **kwargs)

class LogBuffer:
    """æ—¥å¿—ç¼“å­˜ç±»ï¼Œç”¨äºæ”¶é›†å•ä¸ªä»»åŠ¡çš„æ‰€æœ‰æ—¥å¿—"""
    def __init__(self, task_id):
        self.task_id = task_id
        self.logs = []
        self.start_time = datetime.datetime.now()
    
    def log(self, message):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.logs.append(f"[{timestamp}] [{self.task_id}] {message}")
    
    def flush(self):
        """ä¸€æ¬¡æ€§è¾“å‡ºæ‰€æœ‰ç¼“å­˜çš„æ—¥å¿—"""
        with output_lock:
            for log in self.logs:
                print(log)
            print()


class HiPhODataset(ImageBaseDataset):
    """
    HiPhO (High School Physics Olympiad) Benchmark Dataset
    
    æ”¯æŒ13ä¸ªç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›æ•°æ®é›†ï¼š
    - IPhO 2024/2025: å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹
    - EuPhO 2024/2025: æ¬§æ´²ç‰©ç†å¥¥æ—åŒ¹å…‹  
    - APhO 2025: äºšæ´²ç‰©ç†å¥¥æ—åŒ¹å…‹
    - PanPhO 2024/2025: æ³›äºšç‰©ç†å¥¥æ—åŒ¹å…‹
    - NBPhO 2024/2025: åŒ—æ¬§-æ³¢ç½—çš„æµ·ç‰©ç†å¥¥æ—åŒ¹å…‹
    - F_MA 2024/2025: ç¾å›½ç‰©ç†ç«èµ›
    - PanMechanics 2024/2025: æ³›äºšåŠ›å­¦ç«èµ›
    
    é›†æˆäº†hipho_verifieréªŒè¯å™¨ï¼Œæ”¯æŒç²—ç»†ç²’åº¦è¯„æµ‹
    """
    TYPE = 'VQA'  # ç»Ÿä¸€ä½¿ç”¨VQAç±»å‹
    
    # æ•°æ®é›†URLæ˜ å°„ - æŒ‡å‘HuggingFaceæ•°æ®é›†
    DATASET_URL = {
        'IPhO_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'IPhO_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'EuPhO_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'EuPhO_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'APhO_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'PanPhO_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'PanPhO_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'NBPhO_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'NBPhO_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'F_MA_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'F_MA_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'PanMechanics_2024': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
        'PanMechanics_2025': 'https://huggingface.co/datasets/haiyuanwan/HiPhO',
    }
    
    # MD5å€¼æš‚æ—¶è®¾ä¸ºç©ºï¼Œå› ä¸ºHuggingFaceæ•°æ®é›†æ˜¯åŠ¨æ€åŠ è½½çš„
    DATASET_MD5 = {
        'IPhO_2024': '',
        'IPhO_2025': '',
        'EuPhO_2024': '',
        'EuPhO_2025': '',
        'APhO_2025': '',
        'PanPhO_2024': '',
        'PanPhO_2025': '',
        'NBPhO_2024': '',
        'NBPhO_2025': '',
        'F_MA_2024': '',
        'F_MA_2025': '',
        'PanMechanics_2024': '',
        'PanMechanics_2025': '',
    }

    def __init__(self, dataset='IPhO_2025', skip_noimg=False, language='en'):
        """åˆå§‹åŒ–æ•°æ®é›†"""
        super().__init__(dataset=dataset, skip_noimg=skip_noimg)
        self.language = language

    @classmethod
    def supported_datasets(cls):
        return list(cls.DATASET_URL.keys())

    def load_data(self, dataset):
        """ä»HuggingFaceåŠ è½½å¤šsplitæ•°æ®é›†"""
        from datasets import load_dataset
        
        safe_print(f"ä»HuggingFaceåŠ è½½æ•°æ®é›†: haiyuanwan/HiPhO, split: {dataset}")
        
        # ä»HuggingFaceåŠ è½½æŒ‡å®šsplitçš„æ•°æ®é›†
        hf_dataset = load_dataset('haiyuanwan/HiPhO', split=dataset)
        safe_print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œå…± {len(hf_dataset)} è¡Œæ•°æ®")
        
        # è½¬æ¢ä¸ºDataFrame
        data = hf_dataset.to_pandas()
        
        # ç¡®ä¿indexåˆ—å­˜åœ¨
        if 'index' not in data.columns:
            data['index'] = range(len(data))
        
        # å¤„ç†å›¾åƒæ•°æ® - HuggingFaceæ•°æ®é›†ä¸­image_questionåŒ…å«base64æ•°æ®
        if 'image_question' in data.columns:
            safe_print(f"ğŸ–¼ï¸  å‘ç°image_questionåˆ—ï¼Œå¤„ç†base64å›¾åƒæ•°æ®")
            
            # ä½¿ç”¨é•¿åº¦è¶…è¿‡64çš„å ä½ç¬¦æ¥è¡¨ç¤ºæ— å›¾åƒ
            no_image_placeholder = 'NO_IMAGE_PLACEHOLDER_' + 'x' * 50
            
            def process_hf_base64_image(base64_data):
                if pd.isna(base64_data) or not str(base64_data).strip() or len(str(base64_data).strip()) < 100:
                    return no_image_placeholder
                # HuggingFaceä¸­çš„image_questionåŒ…å«base64æ•°æ®ï¼Œç›´æ¥è¿”å›ç”¨äºVLMEvalKitå¤„ç†
                return str(base64_data)
            
            # åˆ›å»ºimageå­—æ®µæ˜ å°„base64æ•°æ®
            data['image'] = data['image_question'].apply(process_hf_base64_image)
            
            # ç»Ÿè®¡å›¾åƒæ•°é‡
            image_count = len(data[~data['image'].str.startswith('NO_IMAGE_PLACEHOLDER_')])
            safe_print(f"ğŸ“ˆ å›¾åƒæ•°æ®ç»Ÿè®¡: {image_count}/{len(data)} æ¡è®°å½•åŒ…å«å›¾åƒ")
        
        safe_print(f"ğŸ“Š æ•°æ®åˆ—å: {list(data.columns)}")
        safe_print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        return data

    def build_prompt(self, line):
        """æ„å»ºè¾“å…¥promptï¼Œå¤„ç†æœ‰å›¾å’Œæ— å›¾ä¸¤ç§æƒ…å†µï¼Œä½¿ç”¨ç‰©ç†ç«èµ›ä¸“ä¸šprompt"""
        if isinstance(line, int):
            line_idx = line
            line = self.data.iloc[line]
            safe_print(f"ğŸ“ æ„å»ºç¬¬ {line_idx+1} é¢˜çš„prompt")
        else:
            safe_print(f"ğŸ“ æ„å»ºprompt (ä½¿ç”¨ä¼ å…¥çš„lineå¯¹è±¡)")

        # ä»æ•°æ®ä¸­è·å–å„ä¸ªå­—æ®µï¼Œå®‰å…¨å¤„ç†å¯èƒ½ä¸ºNaNçš„å­—æ®µ
        def safe_str(val):
            return "" if pd.isna(val) or val == '' else str(val)
        
        context = safe_str(line.get('context', ''))
        question = safe_str(line['question'])
        information = safe_str(line.get('information', ''))
        
        safe_print(f"   ğŸ“‹ é¢˜ç›®ä¿¡æ¯:")
        safe_print(f"      - contexté•¿åº¦: {len(context)} å­—ç¬¦")
        safe_print(f"      - questioné•¿åº¦: {len(question)} å­—ç¬¦")
        safe_print(f"      - informationé•¿åº¦: {len(information)} å­—ç¬¦")
        safe_print(f"      - ä½¿ç”¨è¯­è¨€: {self.language}")
        
        # é€‰æ‹©è¯­è¨€å¯¹åº”çš„promptæ¨¡æ¿
        system_prompt = SYSTEM_PROMPTS_EN if self.language == 'en' else SYSTEM_PROMPTS_ZH
        # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢è€Œä¸æ˜¯formatï¼Œé¿å…èŠ±æ‹¬å·å†²çª
        formatted_prompt = system_prompt.replace('{context}', context).replace('{problem}', question).replace('{information}', information)
        
        safe_print(f"   ğŸ”§ æ„å»ºçš„prompté•¿åº¦: {len(formatted_prompt)} å­—ç¬¦")
        
        msgs = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒæ•°æ®ï¼ˆbase64æˆ–è·¯å¾„ï¼‰
        image_val = str(line.get('image', '')).strip()
        safe_print(f"   ğŸ–¼ï¸  å›¾åƒæ£€æŸ¥: {'æœ‰å›¾åƒ' if image_val and not image_val.startswith('NO_IMAGE_PLACEHOLDER_') else 'æ— å›¾åƒ'}")
        
        if image_val and not image_val.startswith('NO_IMAGE_PLACEHOLDER_'):
            # æ£€æŸ¥æ˜¯å¦æ˜¯base64æ•°æ®
            if len(image_val) > 1000 and not image_val.startswith('/'):  # base64æ•°æ®é€šå¸¸å¾ˆé•¿ä¸”ä¸ä»¥/å¼€å¤´
                safe_print(f"      - æ£€æµ‹åˆ°base64å›¾åƒæ•°æ® (é•¿åº¦: {len(image_val)})")
                # ç›´æ¥ä½¿ç”¨base64æ•°æ®ï¼ŒVLMEvalKitæ¡†æ¶ä¼šå¤„ç†
                msgs.append(dict(type='image', value=image_val))
                safe_print(f"      - æ·»åŠ äº†base64å›¾åƒåˆ°æ¶ˆæ¯åˆ—è¡¨")
            else:
                safe_print(f"      - å›¾åƒè·¯å¾„: {str(image_val)[:50]}{'...' if len(str(image_val)) > 50 else ''}")
                # æœ‰å›¾åƒè·¯å¾„çš„æƒ…å†µ - ä½¿ç”¨æ¡†æ¶çš„æ ‡å‡†å›¾åƒå¤„ç†
                if self.meta_only:
                    tgt_path = toliststr(line['image_path']) if 'image_path' in line else []
                    safe_print(f"      - meta_onlyæ¨¡å¼ï¼Œå›¾åƒè·¯å¾„: {tgt_path}")
                else:
                    safe_print(f"      - å¼€å§‹dumpå›¾åƒ...")
                    tgt_path = self.dump_image(line)
                    safe_print(f"      - dumpç»“æœ: {tgt_path}")
                
                if tgt_path and tgt_path != ['']:
                    if isinstance(tgt_path, list):
                        msgs.extend([dict(type='image', value=p) for p in tgt_path])
                        safe_print(f"      - æ·»åŠ äº† {len(tgt_path)} ä¸ªå›¾åƒåˆ°æ¶ˆæ¯åˆ—è¡¨")
                    else:
                        msgs.append(dict(type='image', value=tgt_path))
                        safe_print(f"      - æ·»åŠ äº† 1 ä¸ªå›¾åƒåˆ°æ¶ˆæ¯åˆ—è¡¨")
        
        # æ·»åŠ æ ¼å¼åŒ–çš„ç‰©ç†ç«èµ›prompt
        msgs.append(dict(type='text', value=formatted_prompt))
        
        safe_print(f"   âœ… promptæ„å»ºå®Œæˆï¼Œæ€»æ¶ˆæ¯æ•°: {len(msgs)} (å›¾åƒ: {len([m for m in msgs if m['type'] == 'image'])}, æ–‡æœ¬: {len([m for m in msgs if m['type'] == 'text'])})")
        return msgs

    def evaluate(self, eval_file, **judge_kwargs):
        """è¯„æµ‹å‡½æ•° - ç»Ÿä¸€çš„ç²—ç»†ç²’åº¦è¯„æµ‹"""
        data = load(eval_file)
        assert 'answer' in data and 'prediction' in data
        
        # è·å–å¹¶è¡Œå‚æ•°
        nproc = judge_kwargs.pop('nproc', 4)
        safe_print(f"ğŸ”§ è®¾ç½®å¹¶è¡Œè¿›ç¨‹æ•°: {nproc}")
        
        # åˆå§‹åŒ–judgeæ¨¡å‹ï¼ˆç”¨äºç»†ç²’åº¦è¯„æµ‹ï¼‰
        judge_model = self._init_judge_model(judge_kwargs)
        
        safe_print(f"ğŸ“Š å¼€å§‹å¹¶è¡Œè¯„æµ‹ï¼Œå…±{len(data)}é¢˜...")
        
        # æ„å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        indices = []
        for i in range(len(data)):
            row = data.iloc[i]
            task_kwargs = judge_kwargs.copy()
            task = (judge_model, row, i, task_kwargs)
            tasks.append(task)
            indices.append(i)
        
        # è®¾ç½®ä¸­é—´ç»“æœä¿å­˜æ–‡ä»¶
        tmp_file = eval_file.replace('.xlsx', '_parallel_tmp.pkl')
        
        # å¹¶è¡Œè¯„æµ‹æ‰€æœ‰é¢˜ç›®
        parallel_results = track_progress_rich(
            self._evaluate_single_problem,
            tasks,
            nproc=nproc,
            chunksize=max(1, nproc//2),
            keys=indices,
            save=tmp_file
        )
        
        safe_print(f"âœ… å¹¶è¡Œè¯„æµ‹å®Œæˆï¼Œå¼€å§‹æ±‡æ€»ç»“æœ...")
        
        # æ±‡æ€»å¹¶è¡Œç»“æœ
        fine_grained_total_score = 0.0
        coarse_grained_total_score = 0.0
        max_possible_score = 0.0
        detailed_results = []
        
        for i, result in enumerate(parallel_results):
            if result is None:
                safe_print(f"âš ï¸  é¢˜ç›® {i+1} è¯„æµ‹å¤±è´¥ï¼Œè·³è¿‡")
                continue
                
            row = data.iloc[i]
            fine_score = result['fine_grained_score']
            coarse_score = result['coarse_grained_score']
            item_points = result['item_total_points']
            
            # ç´¯åŠ å¾—åˆ†
            fine_grained_total_score = round(fine_grained_total_score + fine_score, 2)
            coarse_grained_total_score = round(coarse_grained_total_score + coarse_score, 2)
            max_possible_score += item_points
            
            # æ„å»ºè¯¦ç»†ç»“æœ
            detailed_item = self._build_result_item_from_parallel_result(row, i, result)
            detailed_results.append(detailed_item)
            
            if (i + 1) % 10 == 0 or i == len(parallel_results) - 1:
                safe_print(f"ğŸ“Š æ±‡æ€»è¿›åº¦ {i+1}/{len(parallel_results)}: ç»†ç²’åº¦={fine_grained_total_score:.2f}, ç²—ç²’åº¦={coarse_grained_total_score:.2f}")
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        max_possible_score = round(max_possible_score, 2)
        results = self._build_final_results(fine_grained_total_score, coarse_grained_total_score, max_possible_score, len(data))
        
        # ä¿å­˜ç»“æœ
        self._save_results(eval_file, results, detailed_results, data)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if osp.exists(tmp_file):
                os.remove(tmp_file)
        except Exception as e:
            safe_print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ‰“å°æ€»ç»“å¹¶è¿”å›DataFrameæ ¼å¼ç»“æœ
        self._print_summary(results)
        return results

    def _init_judge_model(self, judge_kwargs):
        """åˆå§‹åŒ–judgeæ¨¡å‹"""
        judge_model_name = judge_kwargs.get('model', None)
        
        if judge_model_name and judge_model_name != 'exact_matching':
            if gpt_key_set():
                try:
                    model_kwargs = {
                        'model': judge_model_name,
                        'timeout': 600,  # è®¾ç½®600ç§’APIçº§åˆ«è¶…æ—¶
                        'retry': 3,      # è®¾ç½®é‡è¯•æ¬¡æ•°
                        'max_tokens': 4096,  # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œå‡å°‘å“åº”æ—¶é—´
                        'verbose': False,  # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…³é—­verboseæ¨¡å¼ï¼Œé¿å…æ‰“å°å®Œæ•´å“åº”
                        **{k: v for k, v in judge_kwargs.items() if k not in ['model', 'nproc']}
                    }
                    test_model = build_judge(**model_kwargs)
                    if test_model.working():
                        safe_print(f"ğŸ¤– ä½¿ç”¨Judgeæ¨¡å‹: {judge_model_name} (timeout=600s, retry=3)")
                        return test_model
                    else:
                        warnings.warn('Judge APIä¸å·¥ä½œï¼Œè·³è¿‡ç»†ç²’åº¦è¯„æµ‹')
                except Exception as e:
                    warnings.warn(f'æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œè·³è¿‡ç»†ç²’åº¦è¯„æµ‹')
            else:
                warnings.warn('API_KEYæ— æ•ˆï¼Œè·³è¿‡ç»†ç²’åº¦è¯„æµ‹')
        
        return None

    def _evaluate_single_problem(self, judge_model, row, index, judge_kwargs):
        """è¯„æµ‹å•ä¸ªé¢˜ç›®çš„å‡½æ•°ï¼ˆç”¨äºå¹¶è¡Œè°ƒç”¨ï¼‰"""
        task_id = f"é¢˜ç›®{index + 1}"
        log_buffer = LogBuffer(task_id)
        
        try:
            log_buffer.log(f"ğŸ“– å¼€å§‹è¯„æµ‹ - ID: {row.get('id', 'N/A')}")
            
            # æå–å­—æ®µ
            prediction = str(row['prediction']).strip()
            ground_truth = self._safe_parse_json_field(row.get('answer', ''))
            answer_type = self._safe_parse_json_field(row.get('answer_type', 'Open-End'))
            unit = self._safe_parse_json_field(row.get('unit', ''))
            points = self._safe_parse_points_field(row.get('points', 0))
            marking = self._safe_parse_json_field(row.get('marking', ''))
            
            item_total_points = sum(points) if points else 0.0
            log_buffer.log(f"   - æœ¬é¢˜æ€»åˆ†: {item_total_points}")
            
            # ç»†ç²’åº¦è¯„æµ‹
            log_buffer.log(f"ğŸ” å¼€å§‹ç»†ç²’åº¦è¯„æµ‹...")
            fine_grained_score, marking_detailed_scores = self._evaluate_fine_grained_with_buffer(
                prediction, marking, points, judge_model, row.get('question', ''), log_buffer
            )
            log_buffer.log(f"âœ… ç»†ç²’åº¦å¾—åˆ†: {fine_grained_score}")
            
            # ç²—ç²’åº¦è¯„æµ‹
            log_buffer.log(f"ğŸ¯ å¼€å§‹ç²—ç²’åº¦è¯„æµ‹...")
            coarse_grained_score, extracted_pred = self._evaluate_coarse_grained_with_buffer(
                prediction, ground_truth, answer_type, unit, points, 
                row.get('question', ''), log_buffer
            )
            log_buffer.log(f"âœ… ç²—ç²’åº¦å¾—åˆ†: {coarse_grained_score}")
            log_buffer.log(f"ğŸ“¤ æå–çš„é¢„æµ‹ç­”æ¡ˆ: {extracted_pred}")
            
            # æœ€ç»ˆå¾—åˆ†å–ä¸¤è€…æœ€å¤§å€¼
            final_score = max(fine_grained_score, coarse_grained_score)
            log_buffer.log(f"ğŸ† æœ€ç»ˆå¾—åˆ†ï¼ˆå–æœ€å¤§å€¼ï¼‰: {final_score} = max({fine_grained_score}, {coarse_grained_score})")
            
            # è¿”å›å•é¢˜ç»“æœ
            result = {
                'index': index,
                'fine_grained_score': fine_grained_score,
                'coarse_grained_score': coarse_grained_score,
                'final_score': final_score,
                'extracted_pred': extracted_pred,
                'marking_detailed_scores': marking_detailed_scores,
                'item_total_points': item_total_points,
                'ground_truth': ground_truth,
                'answer_type': answer_type,
                'unit': unit,
                'points': points,
                'marking': marking,
                'prediction': prediction
            }
            
            log_buffer.log(f"âœ… è¯„æµ‹å®Œæˆï¼Œæœ€ç»ˆå¾—åˆ†: {final_score}")
            log_buffer.flush()
            return result
            
        except Exception as e:
            log_buffer.log(f"âŒ è¯„æµ‹å¤±è´¥: {e}")
            import traceback
            log_buffer.log(f"ğŸ“„ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            log_buffer.flush()
            return None

    def _evaluate_fine_grained_with_buffer(self, prediction, marking, points, judge_model, question, log_buffer):
        """ç»†ç²’åº¦è¯„æµ‹ - å¸¦é‡æµ‹æœºåˆ¶ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        log_buffer.log(f"   ğŸ” ç»†ç²’åº¦è¯„æµ‹å¼€å§‹")
        log_buffer.log(f"      - markingæ•°é‡: {len(marking) if marking else 0}")
        log_buffer.log(f"      - judge_model: {'æœ‰' if judge_model else 'æ— '}")
        
        if not marking or not judge_model:
            log_buffer.log(f"   âš ï¸  è·³è¿‡ç»†ç²’åº¦è¯„æµ‹ï¼š{'æ— markingæ ‡å‡†' if not marking else 'æ— judgeæ¨¡å‹'}")
            return 0.0, []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šå¥—markingæ ‡å‡†
        if self._has_multiple_marking_sets(marking):
            log_buffer.log(f"   ğŸ“‹ å‘ç°å¤šå¥—markingæ ‡å‡†ï¼Œä½¿ç”¨æœ€ä½³å¾—åˆ†ç­–ç•¥")
            return self._evaluate_multiple_marking_sets_with_buffer(prediction, marking, points, judge_model, question, log_buffer)
            
        scoring_criteria = self._parse_marking_criteria(marking)
        max_possible_score = sum(points) if points else 0.0
        max_retries = 3  # æœ€å¤§é‡æµ‹æ¬¡æ•°
        
        log_buffer.log(f"   ğŸ“Š è¯„æµ‹é…ç½®: {len(scoring_criteria)}ä¸ªæ ‡å‡†ï¼Œæœ€å¤§æ€»åˆ†: {max_possible_score}")
        
        for attempt in range(max_retries + 1):
            log_buffer.log(f"   ğŸ”„ å¼€å§‹ç¬¬ {attempt + 1} æ¬¡è¯„æµ‹")
            scores = []
            detailed_scores = []
            
            for i, criterion in enumerate(scoring_criteria):
                score, response = self._evaluate_single_criterion_with_buffer(
                    prediction, criterion, judge_model, question, 
                    max_total_score=max_possible_score, 
                    current_attempt=attempt,
                    log_buffer=log_buffer
                )
                scores.append(score)
                
                detailed_scores.append({
                    'marking_criterion': criterion['description'],
                    'score': round(score, 2),
                    'index': criterion['index'],
                    'attempt': attempt + 1,
                    'judge_response': response
                })
            
            total_score = sum(scores)
            log_buffer.log(f"   ğŸ“Š ç¬¬ {attempt + 1} æ¬¡è¯„æµ‹æ€»åˆ†: {total_score}")
            
            if total_score <= max_possible_score or max_possible_score == 0:
                for detailed_score in detailed_scores:
                    detailed_score['retry_info'] = f"ç¬¬{attempt + 1}æ¬¡è¯„æµ‹æˆåŠŸ" if attempt > 0 else "é¦–æ¬¡è¯„æµ‹æˆåŠŸ"
                    detailed_score['final_success'] = True
                
                log_buffer.log(f"âœ… è¯„æµ‹æˆåŠŸï¼Œæ€»åˆ† {total_score:.2f}")
                return round(total_score, 2), detailed_scores
            else:
                if attempt < max_retries:
                    log_buffer.log(f"âš ï¸  è¯„æµ‹è¶…åˆ†: {total_score:.2f} > {max_possible_score:.2f}ï¼Œé‡æµ‹...")
                else:
                    # å¼ºåˆ¶è°ƒæ•´
                    scale_factor = max_possible_score / total_score
                    adjusted_scores = [score * scale_factor for score in scores]
                    
                    for i, score in enumerate(adjusted_scores):
                        detailed_scores[i]['original_score'] = detailed_scores[i]['score']
                        detailed_scores[i]['score'] = round(score, 2)
                        detailed_scores[i]['forced_adjustment'] = True
                        detailed_scores[i]['scale_factor'] = round(scale_factor, 3)
                    
                    log_buffer.log(f"ğŸ“Š å¼ºåˆ¶è°ƒæ•´åˆ†æ•°ï¼Œç³»æ•°: {scale_factor:.3f}")
                    return round(sum(adjusted_scores), 2), detailed_scores
        
        return 0.0, []

    def _evaluate_coarse_grained_with_buffer(self, prediction, ground_truth, answer_type, unit, points, question, log_buffer):
        """ç²—ç²’åº¦è¯„æµ‹ - åŸºäºphysics_r1éªŒè¯å™¨çš„ç­”æ¡ˆåŒ¹é…"""
        log_buffer.log(f"   ğŸ¯ ç²—ç²’åº¦è¯„æµ‹å¼€å§‹")
        
        extracted_pred = ""
        
        if ground_truth:
            log_buffer.log(f"      âœ… æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå¼€å§‹physics_r1éªŒè¯")
            try:
                # ä½¿ç”¨physics_r1éªŒè¯å™¨
                total_score, total_point, extracted_preds, extracted_gts, scored_by_list = answer_tag_reward_fn_for_r1(
                    prediction, ground_truth, problem=question, points=points, use_xverify=True, debug=False
                )
                
                extracted_pred = ", ".join([str(p) for p in extracted_preds if p])
                log_buffer.log(f"      ğŸ“Š physics_r1éªŒè¯å¾—åˆ†: {total_point}")
                log_buffer.log(f"      ğŸ“ æå–çš„ç­”æ¡ˆ: {extracted_pred}")
                
                return round(total_point, 2), extracted_pred
                
            except Exception as e:
                log_buffer.log(f"      âš ï¸  physics_r1éªŒè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•åŒ¹é…")
                # å›é€€åˆ°ç®€å•åŒ¹é…
                simple_score = self._simple_answer_matching(prediction, ground_truth, points)
                extracted_pred = self._extract_prediction_for_display(prediction)
                return round(simple_score, 2), extracted_pred
        
        log_buffer.log(f"      âš ï¸  æ— æ ‡å‡†ç­”æ¡ˆï¼Œè¿”å›0åˆ†")
        return 0.0, extracted_pred

    def _evaluate_single_criterion_with_buffer(self, prediction, criterion, judge_model, question, max_total_score=None, current_attempt=0, log_buffer=None):
        """ä½¿ç”¨judgeæ¨¡å‹è¯„æµ‹å•ä¸ªmarkingæ ‡å‡†"""
        log_buffer.log(f"         ğŸ¤– è°ƒç”¨Judgeæ¨¡å‹è¯„æµ‹æ ‡å‡†")
        
        # æ„å»ºæ€»åˆ†é™åˆ¶æç¤º
        total_score_warning = ""
        if max_total_score is not None and max_total_score > 0:
            total_score_warning = f"""
âš ï¸  IMPORTANT TOTAL SCORE CONSTRAINT:
- This question has a maximum total score of {max_total_score} points
- ALL marking criteria scores combined MUST NOT exceed {max_total_score} points
- You are evaluating ONE criterion among multiple criteria for this question
- Be conservative in your scoring to ensure the total doesn't exceed the limit
- This is attempt #{current_attempt + 1} of evaluation"""

        retry_warning = ""
        if current_attempt > 0:
            retry_warning = f"""
ğŸ”„ RETRY NOTICE:
- Previous attempt(s) resulted in total score exceeding the maximum
- Please be more conservative in your scoring
- Focus on strict adherence to the criterion requirements"""

        prompt = f"""You are an expert physics competition grader. Evaluate the student's solution against the specific grading criterion.

PHYSICS PROBLEM:
{question}

STUDENT'S SOLUTION:
{prediction}

GRADING CRITERION:
{criterion['description']}{total_score_warning}{retry_warning}

INSTRUCTIONS:
1. Carefully analyze the student's solution for physics concepts, mathematical derivations, and calculations.
2. Compare the solution against the specific grading criterion provided.
3. Award points strictly according to the criterion, including partial credit when specified.
4. Consider both conceptual understanding and technical accuracy.
5. BE CONSERVATIVE - remember this is one of multiple criteria being evaluated simultaneously.

SCORING FORMAT:
- Read the grading criterion carefully to understand the maximum points and conditions for partial credit
- Evaluate whether the student's solution meets the full criteria, partial criteria, or no criteria
- Output your score using the exact format: \\boxed{{score}}
- The score should be a number (e.g., 0.4, 0.2, 0.1, 0.0)

CRITICAL REQUIREMENTS:
- You MUST output your final score in the format: \\boxed{{score}}
- The score must be a single number only (no text inside the boxed)
- Do not include explanations after the boxed score
- Ensure your score follows the point allocation in the grading criterion
- BE CONSERVATIVE to avoid exceeding the total score limit

Example outputs:
- \\boxed{{0.4}} (for full credit)
- \\boxed{{0.1}} (for partial credit)  
- \\boxed{{0.0}} (for no credit)

âš ï¸ CRITICAL INSTRUCTION: 
- Output ONLY: \\boxed{{score}}
- NO explanations, NO analysis, NO reasoning
- Just the number in the exact format \\boxed{{score}}
- Any other text will result in AUTOMATIC REJECTION

RESPOND WITH ONLY THE BOXED SCORE:"""
        
        try:
            start_time = time.time()
            response = judge_model.generate(prompt).strip()
            elapsed_time = time.time() - start_time
            
            log_buffer.log(f"         â±ï¸  å“åº”è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            score = self._extract_score_from_response(response)
            log_buffer.log(f"         ğŸ” æå–çš„åˆ†æ•°: {score}")
            return score, response
        except Exception as e:
            log_buffer.log(f"         âŒ Judgeæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return 0.0, f"Judgeæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

    def _safe_parse_json_field(self, field_value):
        """å®‰å…¨è§£æJSONå­—æ®µ"""
        if pd.isna(field_value) or field_value == '':
            return []
        
        if isinstance(field_value, list):
            return field_value
        
        field_str = str(field_value).strip()
        if field_str.startswith('[') and field_str.endswith(']'):
            try:
                return json.loads(field_str)
            except json.JSONDecodeError:
                return [field_str]
        else:
            return [field_str] if field_str != 'nan' else []
    
    def _safe_parse_points_field(self, points_value):
        """å®‰å…¨è§£æpointså­—æ®µ"""
        if pd.isna(points_value):
            return [0.0]
        
        if isinstance(points_value, list):
            return [float(p) for p in points_value if p is not None]
        
        if isinstance(points_value, (int, float)):
            return [float(points_value)]
        
        points_str = str(points_value).strip()
        if points_str.startswith('[') and points_str.endswith(']'):
            try:
                parsed = json.loads(points_str)
                return [float(p) for p in parsed if p is not None]
            except (json.JSONDecodeError, ValueError):
                pass
        
        try:
            return [float(points_str)]
        except ValueError:
            return [0.0]

    def _has_valid_marking(self, marking):
        """æ£€æŸ¥markingæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„è¯„åˆ†æ ‡å‡†"""
        if not marking:
            return False
        
        if not isinstance(marking, list):
            return False
        
        if len(marking) == 0:
            return False
        
        for item in marking:
            if item is None:
                continue
            
            if isinstance(item, list):
                if len(item) > 0:
                    return True
            elif isinstance(item, str):
                stripped = item.strip()
                if stripped and stripped.lower() not in ['', 'nan', 'none', 'null']:
                    return True
            else:
                return True
        
        return False

    def _has_multiple_marking_sets(self, marking):
        """æ£€æŸ¥æ˜¯å¦æœ‰å¤šå¥—markingæ ‡å‡†"""
        if not marking or len(marking) == 0:
            return False
        
        # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åˆ—è¡¨ï¼Œåˆ™è®¤ä¸ºæœ‰å¤šå¥—æ ‡å‡†
        return isinstance(marking[0], list)
    
    def _evaluate_multiple_marking_sets_with_buffer(self, prediction, marking_sets, points, judge_model, question, log_buffer):
        """è¯„æµ‹å¤šå¥—markingæ ‡å‡†ï¼Œå–æœ€é«˜åˆ†ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        log_buffer.log(f"   ğŸ“‹ å¼€å§‹è¯„æµ‹å¤šå¥—markingæ ‡å‡†ï¼Œå…±{len(marking_sets)}å¥—")
        
        best_score = 0.0
        best_detailed_scores = []
        
        for set_idx, marking_set in enumerate(marking_sets):
            log_buffer.log(f"   ğŸ”„ è¯„æµ‹ç¬¬{set_idx + 1}å¥—markingæ ‡å‡†")
            score, detailed_scores = self._evaluate_single_marking_set_with_buffer(
                prediction, marking_set, points, judge_model, question, log_buffer
            )
            log_buffer.log(f"      ğŸ“Š ç¬¬{set_idx + 1}å¥—å¾—åˆ†: {score}")
            
            # æ›´æ–°æœ€ä½³åˆ†æ•°
            if score > best_score:
                best_score = score
                best_detailed_scores = detailed_scores
                # åœ¨æœ€ä½³è¯¦ç»†å¾—åˆ†ä¸­æ·»åŠ æ ‡è®°
                for detailed_score in best_detailed_scores:
                    detailed_score['best_marking_set'] = set_idx + 1
                log_buffer.log(f"      âœ… æ›´æ–°æœ€ä½³å¾—åˆ†: {best_score} (æ¥è‡ªç¬¬{set_idx + 1}å¥—)")
        
        log_buffer.log(f"   ğŸ† å¤šå¥—markingè¯„æµ‹å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {best_score}")
        return round(best_score, 2), best_detailed_scores
    
    def _evaluate_single_marking_set_with_buffer(self, prediction, marking, points, judge_model, question, log_buffer):
        """è¯„æµ‹å•å¥—markingæ ‡å‡†ï¼ˆå¸¦æ—¥å¿—ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
        scoring_criteria = self._parse_marking_criteria(marking)
        max_possible_score = sum(points) if points else 0.0
        
        scores = []
        detailed_scores = []
        
        for criterion in scoring_criteria:
            score, response = self._evaluate_single_criterion_with_buffer(
                prediction, criterion, judge_model, question, 
                max_total_score=max_possible_score, 
                current_attempt=0,
                log_buffer=log_buffer
            )
            scores.append(score)
            
            # ä¿å­˜æ¯ä¸ªmarkingçš„è¯¦ç»†å¾—åˆ†
            detailed_scores.append({
                'marking_criterion': criterion['description'],
                'score': round(score, 2),
                'index': criterion['index'],
                'judge_response': response
            })
        
        total_score = sum(scores)
        
        # å¦‚æœè¶…è¿‡æœ€å¤§åˆ†æ•°ï¼ŒæŒ‰æ¯”ä¾‹è°ƒæ•´
        if total_score > max_possible_score and max_possible_score > 0:
            scale_factor = max_possible_score / total_score
            total_score = max_possible_score
            for detailed_score in detailed_scores:
                detailed_score['original_score'] = detailed_score['score']
                detailed_score['score'] = round(detailed_score['score'] * scale_factor, 2)
                detailed_score['scaled'] = True
        
        return round(total_score, 2), detailed_scores

    def _parse_marking_criteria(self, marking_list):
        """è§£æmarkingè¯„åˆ†æ ‡å‡†"""
        criteria = []
        if not marking_list:
            return criteria
        
        # å¤„ç†åµŒå¥—åˆ—è¡¨çš„æƒ…å†µ
        flattened_marking = []
        for item in marking_list:
            if isinstance(item, list):
                flattened_marking.extend(item)
            else:
                flattened_marking.append(item)
        
        for i, marking_criterion in enumerate(flattened_marking):
            if marking_criterion and str(marking_criterion).strip():
                criteria.append({
                    'description': str(marking_criterion).strip(),
                    'index': i
                })
        
        return criteria

    def _extract_score_from_response(self, response):
        """ä»æ¨¡å‹å“åº”ä¸­æå–åˆ†æ•°"""
        if not response:
            return 0.0
            
        response = response.strip()
        
        # ä½¿ç”¨boxedæ ¼å¼æå–åˆ†æ•°
        boxed_patterns = [
            r'\\boxed\{([^}]+)\}',
            r'boxed\{([^}]+)\}',
        ]
        
        for pattern in boxed_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            for match in reversed(matches):
                match = match.strip()
                if match:
                    try:
                        score = float(match)
                        return round(score, 2)
                    except ValueError:
                        nums = re.findall(r'\d+\.?\d*', match)
                        if nums:
                            try:
                                score = float(nums[-1])
                                return round(score, 2)
                            except ValueError:
                                continue
        
        # æŸ¥æ‰¾æ•°å­—
        all_numbers = re.findall(r'[0-9]*\.?[0-9]+', response)
        if all_numbers:
            try:
                score = float(all_numbers[-1])
                return round(score, 2)
            except ValueError:
                pass
        
        return 0.0

    def _simple_answer_matching(self, prediction, answer_list, points_list):
        """ç®€å•çš„ç­”æ¡ˆåŒ¹é…ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        total_score = 0.0
        
        for gt, points in zip(answer_list, points_list):
            if gt and gt.strip():
                if str(gt).strip().lower() in prediction.lower():
                    total_score += points
        
        return total_score
    
    def _extract_prediction_for_display(self, prediction, num_answers=10):
        """æå–é¢„æµ‹ç­”æ¡ˆç”¨äºæ˜¾ç¤º"""
        try:
            extracted_answers = get_answer_str(prediction, return_origin=False, num_answers=num_answers)
            valid_answers = []
            
            for ans in extracted_answers:
                if ans and ans.strip():
                    cleaned_ans = ' '.join(ans.strip().replace('\n', ' ').replace('\r', ' ').split())
                    if cleaned_ans:
                        valid_answers.append(cleaned_ans)
            
            return ", ".join(valid_answers) if valid_answers else ""
        except Exception:
            try:
                extracted = extract_boxed_answer(prediction)
                if extracted and extracted.strip():
                    cleaned = ' '.join(extracted.strip().replace('\n', ' ').replace('\r', ' ').split())
                    return cleaned if cleaned else ""
            except Exception:
                pass
            return ""

    def _build_result_item_from_parallel_result(self, row, index, parallel_result):
        """ä»å¹¶è¡Œç»“æœæ„å»ºè¯¦ç»†ç»“æœé¡¹"""
        has_marking = parallel_result['marking'] and len(parallel_result['marking']) > 0 and self._has_valid_marking(parallel_result['marking'])
        earned_points = max(parallel_result['fine_grained_score'], parallel_result['coarse_grained_score'])
        
        return {
            "id": str(row.get('id', f"{self.dataset_name}_{index+1}")),
            "context": str(row.get('context', '')).strip(),
            "question": str(row.get('question', '')).strip(),
            "solution": str(row.get('solution', '')).strip(),
            "marking": parallel_result['marking'] if parallel_result['marking'] else [],
            "marking_detailed_scores": parallel_result['marking_detailed_scores'],
            "answer": [f"\\boxed{{{ans}}}" for ans in parallel_result['ground_truth']] if parallel_result['ground_truth'] else [''],
            "answer_type": parallel_result['answer_type'] if parallel_result['answer_type'] else ['Open-End'],
            "unit": parallel_result['unit'] if parallel_result['unit'] else [''],
            "points": parallel_result['points'] if parallel_result['points'] else [0.0],
            "modality": str(row.get('modality', 'text')).strip(),
            "field": str(row.get('field', '')).strip(),
            "source": self.dataset_name,
            "test_result": str(parallel_result['prediction']),
            "test_answer": [f"\\boxed{{{ans.strip()}}}" for ans in parallel_result['extracted_pred'].split(", ") if ans.strip()] if parallel_result['extracted_pred'] else [''],
            "fine_grained_score": parallel_result['fine_grained_score'],
            "coarse_grained_score": parallel_result['coarse_grained_score'],
            "earned_points": earned_points
        }

    def _build_final_results(self, fine_total, coarse_total, max_score, total_count):
        """æ„å»ºæœ€ç»ˆç»“æœ"""
        fine_rate = round((fine_total / max_score * 100), 2) if max_score > 0 else 0.0
        coarse_rate = round((coarse_total / max_score * 100), 2) if max_score > 0 else 0.0
        
        return {
            'fine_grained_total_score': fine_total,
            'fine_grained_score_rate': fine_rate,
            'fine_grained_count': total_count,  # æ·»åŠ ç¼ºå°‘çš„å­—æ®µ
            'coarse_grained_total_score': coarse_total,
            'coarse_grained_score_rate': coarse_rate,
            'coarse_grained_count': total_count,  # æ·»åŠ ç¼ºå°‘çš„å­—æ®µ
            'max_possible_score': max_score,
            'total_count': total_count,
            'total_score': fine_total,
            'score_rate': fine_rate,
        }

    def _save_results(self, eval_file, results, detailed_results, data):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        score_file = eval_file.replace('.xlsx', '_score.json')
        detailed_file = eval_file.replace('.xlsx', '_detailed_results.json')
        detailed_xlsx_file = eval_file.replace('.xlsx', '_detailed.xlsx')
        
        dump(results, score_file)
        dump(detailed_results, detailed_file)
        
        try:
            eval_data_with_results = data.copy()
            eval_data_with_results['fine_grained_score'] = [r['fine_grained_score'] for r in detailed_results]
            eval_data_with_results['coarse_grained_score'] = [r['coarse_grained_score'] for r in detailed_results]
            eval_data_with_results['earned_points'] = [r['earned_points'] for r in detailed_results]
            eval_data_with_results['marking_detailed_scores'] = [
                json.dumps(r['marking_detailed_scores'], ensure_ascii=False) if r['marking_detailed_scores'] else '[]' 
                for r in detailed_results
            ]
            dump(eval_data_with_results, detailed_xlsx_file)
        except Exception as e:
            safe_print(f"âš ï¸  ä¿å­˜è¯¦ç»†Excelæ–‡ä»¶å¤±è´¥: {e}")

    def _print_summary(self, results):
        """æ‰“å°è¯„æµ‹æ€»ç»“"""
        safe_print(f"âœ… HiPhOæ•°æ®é›†è¯„ä¼°å®Œæˆï¼")
        safe_print(f"ğŸ† æ€»ä½“å¾—åˆ†: {results['total_score']:.2f} / {results['max_possible_score']:.2f} ({results['score_rate']:.2f}%)")
        safe_print(f"ğŸ“Š ç»†ç²’åº¦è¯„æµ‹: {results['fine_grained_count']}é¢˜ï¼Œå¾—åˆ† {results['fine_grained_total_score']:.2f} ({results['fine_grained_score_rate']:.2f}%)")
        safe_print(f"ğŸ¯ ç²—ç²’åº¦è¯„æµ‹: {results['coarse_grained_count']}é¢˜ï¼Œå¾—åˆ† {results['coarse_grained_total_score']:.2f} ({results['coarse_grained_score_rate']:.2f}%)")
        safe_print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜")