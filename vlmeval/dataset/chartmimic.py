import re
import os
import sys

from ..smp import *
logger = get_logger("ChartMimic")

# SET VLMEVAL_CHARTMIMIC_UTILS_PATH for chartmimic evaluator
# ".../VLMEvalKit/vlmeval..."
cur_path = os.path.abspath(__file__)
# get path before "VLMEvalKit/vlmeval", then add "VLMEvalKit/vlmeval"
vlmeval_path = cur_path.split("VLMEvalKit/vlmeval")[0] + "VLMEvalKit/vlmeval/"
os.environ["VLMEVAL_CHARTMIMIC_UTILS_PATH"] = vlmeval_path + "dataset/utils/chartmimic"
if os.environ["VLMEVAL_CHARTMIMIC_UTILS_PATH"] not in sys.path:
    sys.path.insert(0, os.environ["VLMEVAL_CHARTMIMIC_UTILS_PATH"])
    logger.info(f"sys.path add: {os.environ['VLMEVAL_CHARTMIMIC_UTILS_PATH']}")
    # logger.info(f"sys.path: {sys.path}")

from .image_base import ImageBaseDataset
from .utils import build_judge, DEBUG_MESSAGE
from ..utils import track_progress_rich
from ..dataset.utils.chartmimic.evaluator.text_evaluator import TextEvaluator
from ..dataset.utils.chartmimic.evaluator.chart_type_evaluator import ChartTypeEvaluator
from ..dataset.utils.chartmimic.evaluator.color_evaluator import ColorEvaluator
from ..dataset.utils.chartmimic.evaluator.layout_evaluator import LayoutEvaluator
# from ..dataset.utils.chartmimic.evaluator.legend_evaluator import LegendEvaluator
# from ..dataset.utils.chartmimic.evaluator.grid_evaluator import GridEvaluator

judge_model = None
save_code_dir = None
sub_set_name = None
cur_work_dir = None
pdf_tmp_dir = None
# save_dir_name_map = {
#     "Direct Mimic": "direct",
#     "Customized Mimic": "customized",
# }

high_level_eval_prompt = {
    "instruction": "You are an excellent judge at evaluating visualization chart plots. The first image (reference image) is created using ground truth matplotlib code, and the second image (AI-generated image) is created using matplotlib code generated by an AI assistant. Your task is to score how well the AI-generated plot matches the ground truth plot.\n\n### Scoring Methodology:\nThe AI-generated image's score is based on the following criteria, totaling a score out of 100 points:\n\n1. **Chart Types (20 points)** Does the AI-generated image include all chart types present in the reference image (e.g., line charts, bar charts, etc.)?\n2. **Layout (10 points)** Does the arrangement of subplots in the AI-generated image match the reference image (e.g., number of rows and columns)?\n3. **Text Content (20 points)** Does the AI-generated image include all text from the reference image (e.g., titles, annotations, axis labels), excluding axis tick labels?\n4. **Data (20 points)** How accurately do the data trends in the AI-generated image resemble those in the original image and is the number of data groups the same as in the reference image?\n5. **Style (20 points)** Does the AI-generated image match the original in terms of colors (line colors, fill colors, etc.), marker types (point shapes, line styles, etc.), legends, grids, and other stylistic details?\n6. **Clarity (10 points)** Is the AI-generated image clear and free of overlapping elements?\n\n### Evaluation:\nCompare the two images head to head and provide a detailed assessment. Use the following format for your response:\n\n\n---\n\nComments:\n- Chart Types: ${your comment and subscore}\n- Layout: ${your comment and subscore}\n- Text Content: ${your comment and subscore}\n- Data: ${your comment and subscore}\n- Style: ${your comment and subscore}\n- Clarity: ${your comment and subscore}\n\nScore: ${your final score out of 100}\n\n---\n\nPlease use the above format to ensure the evaluation is clear and comprehensive.\n",
    "system_msg": ""
}

def image_path_to_data_uri(image_path):
    mime, _ = mimetypes.guess_type(image_path)
    if not mime:
        raise ValueError(f"Cannot determine MIME type for {image_path}")
    with open(image_path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
    return f"data:{mime};base64,{encoded}"

def run_once_with_images(pt, image_abs_path_list, retry=4):
    global judge_model
    # prefix = "data:image/jpeg;base64,"
    # img = prefix + image
    messages = [
        *[dict(type="image", value=image_path_to_data_uri(image_abs_path)) for image_abs_path in image_abs_path_list],
        dict(type="text", value=pt),
    ]
    ans = None
    while retry:
        try:
            ans = judge_model.generate(messages)
            return ans
        except Exception as e:
            logger.exception("Error in run_once_with_images:")
            retry -= 1
    return ans

# def run_once_without_image(pt, retry=3):
#     global judge_model
#     messages = [
#         dict(type="text", value=pt),
#     ]
#     while retry:
#         try:
#             ans = judge_model.generate(messages)
#             return ans
#         except Exception as e:
#             logger.info(f"Error in run_once_without_image: {e}")
#             retry -= 1
#     return ans

# >>> util1: extract python code from markdown text <<<
def extract_python_code(text):
    """Extract python code from markdown text."""
    code_matches = re.findall(r"```python(.*?)```", text, re.DOTALL)
    if not code_matches:
        return ""  # Return an empty string if no code block is found
    return code_matches[0]  # Return the first match

# >>> util2: clean escape characters in code string <<<
def clean_escape_chars(code: str) -> str:
    """
    Clean escape characters in code string to ensure proper execution.
    Handles common escape sequences and ensures proper string formatting.
    
    Args:
        code (str): The code string to clean
        
    Returns:
        str: Cleaned code string
    """
    # Common escape sequences to handle
    escape_map = {
        r'\\n': '\n',      # Newline
        r'\\r': '\r',      # Carriage return
        r'\\t': '\t',      # Tab
        r'\\"': '"',       # Double quote
        r"\\'": "'",       # Single quote
        r'\\\\': '\\',     # Backslash
        r'\\b': '\b',      # Backspace
        r'\\f': '\f',      # Form feed
        r'\\v': '\v',      # Vertical tab
    }
    
    # Replace escape sequences
    for escaped, unescaped in escape_map.items():
        code = code.replace(escaped, unescaped)
    
    return code

def _convert_single_page_pdf_to_png(pdf_path, output_path, dpi=350):
    from pdf2image import convert_from_path
    images = convert_from_path(pdf_path, dpi=dpi)
    images[0].save(output_path, "PNG")

def extract_gpt_score(resp):
    m = re.search(r'^\s*Score:\s*(\d+)\s*/\s*100', resp, re.IGNORECASE | re.MULTILINE)
    return int(m.group(1)) if m else 0

def judge_one_item(item):
    score_dict = {}
    global judge_model, save_code_dir, sub_set_name
    item = json.loads(item)
    # >>> 1. Run Code to Generate PY and PDF <<<
    # extract python code from item["prediction"]
    code = extract_python_code(item["prediction"])
    # clean code string: \\n -> \n...
    code = clean_escape_chars(code)
    # TODO: if len(code) == 0?
    # save code to py and run to generate pdf
    # logger.info(f"save_code_dir: {save_code_dir}")
    if "direct" in item["task"].lower():
        save_dir_name = "direct"
    elif "customized" in item["task"].lower():
        save_dir_name = "customized"
    else:
        raise ValueError(f"Invalid task: {item['task']}")
    output_py = f"{save_code_dir}/ChartMimic/{sub_set_name}/{save_dir_name}/{item['index']}.py"
    os.makedirs(os.path.dirname(output_py), exist_ok=True)
    # clean & add self redefined path
    code = re.sub(r"plt\.savefig\(.*\n*", "", code, flags=re.S)
    code = re.sub(r"plt.show\(.*\n*", "", code, flags=re.S)
    code = (
        code.strip()
        + '\nplt.savefig("{}")'.format(
            output_py.replace(".py", f".pdf")
        )
    )
    with open(output_py, "w") as f:
        f.write(code)
    # [Attention] run code with timeout, enhancement here
    # try generate pdf
    try:
        _ = subprocess.run(
            ["python", output_py],
            timeout=20,  # 20 seconds timeout
            capture_output=True,
            text=True
        )
    except subprocess.TimeoutExpired:
        logger.info("The subprocess timed out.")
    except subprocess.CalledProcessError as e:
        logger.info(f"Subprocess failed with error: {e}")
    except Exception as e:
        logger.info(f"An unexpected error occurred: {e}")

    # try generate image (converted from pdf)
    if os.path.exists(output_py.replace(".py", ".pdf")):
        _convert_single_page_pdf_to_png(output_py.replace(".py", ".pdf"), output_py.replace(".py", ".png"))
        logger.info(f"converted pdf to image: {output_py.replace('.py', '.png')}")
        # breakpoint()

    # --- Got py and its pdf ---
    # >>> 2. Low Level Evaluation <<<
    text_evaluator = TextEvaluator(use_position=False, use_axs=False)
    chart_type_evaluator = ChartTypeEvaluator()
    color_evaluator = ColorEvaluator()
    layout_evaluator = LayoutEvaluator()
    # unused
    # legend_evaluator = LegendEvaluator(use_position=True)
    # grid_evaluator = GridEvaluator()

    ground_truth_figure_code_file_rel = item["ground_truth_figure_code"]

    ROOT = LMUDataRoot()
    img_root = os.path.join(ROOT, 'images', 'ChartMimic')
    ground_truth_figure_code_file = os.path.join(img_root, ground_truth_figure_code_file_rel)

    original_py_file = ground_truth_figure_code_file
    generated_py_file = output_py

    # logger.info(f"original_py_file: {original_py_file}")
    # logger.info(f"generated_py_file: {generated_py_file}")

    # global pdf_tmp_dir
    # os.chdir(pdf_tmp_dir)
    
    text_evaluator(
        generation_code_file=generated_py_file,
        golden_code_file=original_py_file
        )

    chart_type_evaluator(
        generation_code_file=generated_py_file,
        golden_code_file=original_py_file
    )

    color_evaluator(
        generation_code_file=generated_py_file,
        golden_code_file=original_py_file
    )

    layout_evaluator(
        generation_code_file=generated_py_file,
        golden_code_file=original_py_file
    )

    low_level_score_dict = {
        "original_py_file": original_py_file,
        "generated_py_file": generated_py_file,
        "text_metrics": text_evaluator.metrics,
        "chart_type_metrics": chart_type_evaluator.metrics,
        "layout_metrics": layout_evaluator.metrics,
        "color_metrics": color_evaluator.metrics
    }
    # global cur_work_dir
    # os.chdir(cur_work_dir)

    score_dict["low_level"] = low_level_score_dict
    # logger.info(low_level_score_dict)
    # breakpoint()


    # >>> 3. High Level Evaluation <<<
    # generated_pdf_file = generated_py_file.replace(".py", ".pdf")
    # # check if generated_pdf_file exists
    # if not os.path.exists(generated_pdf_file):
    #     logger.info(f"Generated PDF file {generated_pdf_file} does not exist")
    #     score_dict["high_level"] = {
    #         "resp": None,
    #         "msg": "Generated PDF file does not exist",
    #         "score": 0.0
    #     }
    #     return 0, score_dict
    
    # pdf exsits
    # convert pdf to image
    # try:
    # _convert_single_page_pdf_to_png(generated_pdf_file, generated_pdf_file.replace(".pdf", ".png"))
    # logger.info(f"converted pdf to image: {generated_pdf_file.replace('.pdf', '.png')}")
    # breakpoint()
    # [Attention] Unable to get page count. Is poppler installed and in PATH?
    # Solution：apt install poppler-utils
    
    generated_pdf_image_file = generated_py_file.replace(".py", ".png")
    # check if generated_pdf_image_file exists
    if not os.path.exists(generated_pdf_image_file):
        # logger.info(f"Generated PDF image file {generated_pdf_image_file} does not exist")
        score_dict["high_level"] = {
            "resp": None,
            "msg": "Generated PDF image file does not exist",
            "score": 0.0
        }
        logger.info(f"index: {item['index']}, return 0, score_dict: {score_dict}")
        return 0, score_dict
    
    # image order should align with prompt
    resp = run_once_with_images(high_level_eval_prompt["instruction"], [original_py_file.replace(".py", ".png"), generated_pdf_image_file])
    if resp is None:
        logger.error(f"Error in getting response from judge model!")
        score_dict["high_level"] = {
            "resp": None,
            "msg": "Error in getting response from judge model!",
            "score": 0.0
        }
        logger.info(f"index: {item['index']}, return -1, score_dict: {score_dict}")
        return -1, score_dict
    else:
        # logger.info(f"Successfully got response from judge model:\n{resp}")
        score_dict["high_level"] = {
            "resp": resp,
            "msg": "Successfully got response from judge model!",
            "score": extract_gpt_score(resp)
        }
        logger.info(f"index: {item['index']}, return 0, score_dict: {score_dict}")
    return 0, score_dict


class ChartMimic(ImageBaseDataset):
    TYPE = "VQA"

    # TODO: add dataset url and md5
    DATASET_URL = {
        "ChartMimic_v1_customized": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v1_customized.tsv',
        "ChartMimic_v1_direct": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v1_direct.tsv',
        # v2
        "ChartMimic_v2_customized": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v2_customized.tsv',
        "ChartMimic_v2_direct": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v2_direct.tsv',
        "ChartMimic_v2_customized_temp_32": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v2_customized_temp_32.tsv',
        "ChartMimic_v2_customized_temp_4": 'https://opencompass.openxlab.space/utils/VLMEval/ChartMimic_v2_customized_temp_4.tsv'
    }
    DATASET_MD5 = {
        "ChartMimic_v1_customized": None,
        "ChartMimic_v1_direct": None,
        "ChartMimic_v2_customized": None,
        "ChartMimic_v2_direct": None,
        "ChartMimic_v2_customized_temp_32": None,
        "ChartMimic_v2_customized_temp_4": None
    }

    # Given one data record, return the built prompt (a multi-modal message), can override
    # Actually, all lines have single image
    def build_prompt(self, line):
        if isinstance(line, int):
            line = self.data.iloc[line]

        # no "image" in tsv, so self.meta_only is True
        # logger.info(f"self.meta_only: {self.meta_only}")
        # logger.info(line.keys())

        input_figure_path_rel = line["input_figure"]
        instruction = line["question"]

        ROOT = LMUDataRoot()
        img_root = os.path.join(ROOT, 'images', 'ChartMimic')
        input_figure_path = os.path.join(img_root, input_figure_path_rel)


        msgs = []
        msgs = [dict(type="image", value=input_figure_path)]
        msgs = [dict(type="text", value=instruction)] + msgs

        return msgs

    def evaluate(self, eval_file, **judge_kwargs):
        # Way to access subset dataset name, for example, ChartMimic_1000
        # logger.info(f"Evaluating for {self.dataset_name}")
        # breakpoint()

        # return {"score": 99.99}
        # raw_tsv_data = ChartMimic(self.dataset).data
        try:
            from pdf2image import convert_from_path
            from colormath.color_objects import sRGBColor, LabColor
        except ImportError as e:
            logging.critical('Please follow the requirements (see vlmeval/dataset/utils/chartmimic/eval_req.txt) \
                             to install dependency package for chartmimic evaluation.')
            raise e
        infer_data_all = load(eval_file).to_dict(orient="records")

        suffix = eval_file.split(".")[-1]
        infer_model = judge_kwargs["model"]
        storage = os.path.abspath(eval_file.replace(f".{suffix}", f"_{infer_model}.jsonl"))
        score_file = os.path.abspath(eval_file.replace(f".{suffix}", f"_{infer_model}_score.csv"))
        # use abs path because of using os.chdir()
        tmp_file = os.path.abspath(eval_file.replace(f".{suffix}", f"_{infer_model}_tmp.pkl"))
        # actually the --api-nproc
        nproc = judge_kwargs.pop("nproc", 8)
        logger.info(f"nproc: {nproc}")
        global save_code_dir, sub_set_name
        # [Attention] should use absolute dir here
        eval_file_abs_path = os.path.abspath(eval_file)
        save_code_dir = os.path.dirname(eval_file_abs_path)

        # dataset_name is subset name like ChartMimic_1000
        sub_set_name = self.dataset_name

        # params prepare for track_progress_rich
        params_all = [json.dumps(item) for item in infer_data_all]
        indices_all = [line["index"] for line in infer_data_all]

        ans = {}
        if os.path.exists(tmp_file):
            tmp_data = load(tmp_file)
            for k, v in tmp_data.items():
                # -1 means error for getting response from judge model, so try to rejudge for this item
                if v[0] == 0:
                    ans[k] = v
            logger.info(f"Tmp file exists, loaded {len(ans)} data from {tmp_file}")
            # logger.info(f"ans: {ans}")

        tups = [x for x, i in zip(params_all, indices_all) if i not in ans]
        indices = [i for i in indices_all if i not in ans]

        # save current work dir
        global cur_work_dir, pdf_tmp_dir
        cur_work_dir = os.getcwd()
        pdf_tmp_dir = os.path.join(save_code_dir, "chart_mimic_tmp", f"{sub_set_name}")
        os.makedirs(pdf_tmp_dir, exist_ok=True)
        os.chdir(pdf_tmp_dir)

        # >>> judge <<<
        if not osp.exists(storage):
            # judge_kwargs['system_prompt'] = SYSTEM_PROMPT
            judge_kwargs["temperature"] = 0
            judge_kwargs["img_detail"] = "high"
            judge_kwargs["timeout"] = 300
            global judge_model
            judge_model = build_judge(max_tokens=1024, **judge_kwargs)

            assert judge_model.working(), "ChartMimic evaluation requires a working OPENAI API\n" + DEBUG_MESSAGE

            if len(indices):
                new_results = track_progress_rich(
                    judge_one_item,
                    tups,
                    nproc=nproc,
                    chunksize=nproc,
                    keys=indices,
                    save=tmp_file,
                )
                for k, v in zip(indices, new_results):
                    ans[k] = v
            else:
                for k, v in ans.items():
                    ans[k] = v
            
            for item in infer_data_all:
                # ans[i] is a tuple, (0 / -1, score_dict), only use score_dict
                item["judge_result"] = ans[item["index"]][1]

            # storage is a jsonl file
            with open(storage, "w") as f:
                for item in infer_data_all:
                    f.write(json.dumps(item) + "\n")

        # judge finished, rm tmp dir
        os.chdir(cur_work_dir)
        if os.path.exists(pdf_tmp_dir):
            shutil.rmtree(pdf_tmp_dir)
        # breakpoint()

        # logger.info(f"storage: {storage}")
        eval_data_all = load(storage)
        # result_df = pd.DataFrame(columns=["example_count", "exec_rate", "text_score","layout_score",  "type_score",  "color_score", "average", f"gpt_score({judge_kwargs['model']})", "overall"])

        new_result = {
            "example_count": len(eval_data_all),
        }

        # calculate exec_rate by counting pdf file / total num
        denominator = len(eval_data_all)
        pdf_file_cnt = 0
        for item in eval_data_all:
            if os.path.exists(item["judge_result"]["low_level"]["generated_py_file"].replace(".py", ".pdf")):
                pdf_file_cnt += 1
        new_result["exec_rate"] = pdf_file_cnt / denominator * 100
        
        # calculate text_score
        text_score_sum = 0
        for item in eval_data_all:
            text_score_sum += item["judge_result"]["low_level"]["text_metrics"]["f1"]
        new_result["text_score"] = text_score_sum / denominator * 100
        
        # calculate layout_score
        layout_score_sum = 0
        for item in eval_data_all:
            layout_score_sum += item["judge_result"]["low_level"]["layout_metrics"]["f1"]
        new_result["layout_score"] = layout_score_sum / denominator * 100
        
        # calculate type_score
        type_score_sum = 0
        for item in eval_data_all:
            type_score_sum += item["judge_result"]["low_level"]["chart_type_metrics"]["f1"]
        new_result["chart_type_score"] = type_score_sum / denominator * 100

        # calculate color_score
        color_score_sum = 0
        for item in eval_data_all:
            color_score_sum += item["judge_result"]["low_level"]["color_metrics"]["f1"]
        new_result["color_score"] = color_score_sum / denominator * 100

        # calculate average
        new_result["average"] = (new_result["text_score"] + new_result["layout_score"] + new_result["chart_type_score"] + new_result["color_score"]) / 4

        # calculate gpt_score
        gpt_score_sum = 0
        # first extract gpt score from resp
        for item in eval_data_all:
            gpt_score_sum += item["judge_result"]["high_level"]["score"]
        new_result["gpt_score"] = gpt_score_sum / denominator
        
        # calculate overall
        # mean of average and gpt_score
        new_result["overall"] = (new_result["average"] + new_result["gpt_score"]) / 2

        score_df = pd.DataFrame([new_result])
        dump(score_df, score_file)

        return score_df
