# MathVista Results

> - We report the evaluation results on MathVista **TestMini**, which include 1000 test samples. 
>
> - We adopt `GPT-4-Turbo (1106)` as the answer extractor when we failed to extract the answer with heuristic matching. 
> - The performance of **Human  (High school)** and **Random Choice** are copied from the official leaderboard. 
>
> **Category Definitions:** **FQA:** figure QA, **GPS:** geometry problem solving, **MWP:** math word problem, **TQA:** textbook QA, **VQA:** visual QA, **ALG:** algebraic, **ARI:** arithmetic, **GEO:** geometry, **LOG:** logical , **NUM:** numeric, **SCI:** scientific, **STA:** statistical.

## Evaluation Results

| Model                         |   ALL |   SCI |   TQA |   NUM |   ARI |   VQA |   GEO |   ALG |   GPS |   MWP |   LOG |   FQA |   STA |
|:------------------------------|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|
| **Human (High School)**           |  60.3 |  64.9 |  63.2 |  53.8 |  59.2 |  55.9 |  51.4 |  50.9 |  48.4 |  73   |  40.7 |  59.7 |  63.9 |
| GPT-4v (detail: low)          |  47.5 |  62.3 |  67.1 |  22.9 |  45.6 |  38.5 |  50.2 |  53.4 |  50   |  57   |  16.2 |  33.5 |  45.5 |
| GeminiProVision               |  45.7 |  57.4 |  60.8 |  27.1 |  41.9 |  40.2 |  39.7 |  42.3 |  38.5 |  45.7 |  10.8 |  46.1 |  52.5 |
| CogVLM-17B-Chat               |  34.6 |  50.8 |  44.9 |  23.6 |  31.2 |  36.3 |  26.4 |  28.1 |  26   |  27.4 |  16.2 |  39   |  42.2 |
| Qwen-VL-Chat                  |  33.5 |  41   |  39.2 |  24.3 |  28   |  32.4 |  28.5 |  30.2 |  29.8 |  25.8 |  13.5 |  39   |  40.9 |
| InternLM-XComposer-VL         |  29.3 |  36.9 |  37.3 |  27.8 |  28.3 |  34.1 |  31.4 |  28.1 |  28.8 |  29   |  13.5 |  21.9 |  21.9 |
| SharedCaptioner               |  28.9 |  37.7 |  37.3 |  34.7 |  28.3 |  33   |  25.9 |  23.8 |  22.1 |  36   |  16.2 |  21.6 |  20.9 |
| LLaVA-v1.5-13B                |  26.5 |  37.7 |  38.6 |  22.9 |  24.9 |  32.4 |  22.6 |  24.2 |  22.6 |  18.8 |  21.6 |  23.8 |  23.9 |
| LLaVA-InternLM-7B (LoRA)      |  26.4 |  32   |  34.8 |  20.1 |  22.1 |  29.6 |  27.6 |  28.1 |  27.9 |  21   |  24.3 |  21.9 |  20.3 |
| LLaVA-v1.5-13B (LoRA, XTuner) |  26.3 |  44.3 |  39.9 |  20.1 |  24.1 |  32.4 |  20.9 |  22.4 |  21.6 |  18.8 |  18.9 |  23   |  22.9 |
| ShareGPT4V-7B                 |  25.8 |  41   |  38.6 |  19.4 |  25.5 |  36.3 |  19.7 |  21.4 |  20.2 |  16.1 |  13.5 |  22.3 |  21.6 |
| IDEFICS-80B-Instruct          |  25.7 |  37.7 |  34.8 |  22.9 |  25.5 |  33.5 |  20.9 |  20.6 |  20.2 |  21.5 |  18.9 |  22.3 |  21.3 |
| TransCore-M                   |  25.4 |  41   |  44.3 |  19.4 |  24.4 |  34.1 |  21.3 |  24.6 |  20.7 |  17.2 |  13.5 |  17.8 |  18.9 |
| mPLUG-Owl2                    |  25.3 |  44.3 |  41.8 |  18.8 |  23.5 |  31.8 |  18.8 |  20.3 |  17.8 |  16.7 |  13.5 |  23   |  23.9 |
| PandaGPT-13B                  |  24.5 |  35.2 |  30.4 |  17.4 |  21   |  27.4 |  23.8 |  23.8 |  25.5 |  18.8 |  16.2 |  22.3 |  21.6 |
| LLaVA-v1.5-7B (LoRA, XTuner)  |  24.1 |  39.3 |  35.4 |  17.4 |  22.1 |  30.2 |  21.3 |  21   |  21.6 |  16.1 |  24.3 |  20.8 |  20.3 |
| LLaVA-v1-7B                   |  23.8 |  32.8 |  34.2 |  13.9 |  20.7 |  28.5 |  22.2 |  24.6 |  24   |  13.4 |  10.8 |  21.6 |  20.3 |
| InstructBLIP-7B               |  23.6 |  32.8 |  31.6 |  13.9 |  23.5 |  29.6 |  19.7 |  20.6 |  20.2 |  15.6 |  13.5 |  23   |  20.9 |
| LLaVA-v1.5-7B                 |  23.6 |  33.6 |  36.7 |  11.1 |  21   |  28.5 |  18.8 |  23.1 |  19.2 |  14.5 |  13.5 |  22.3 |  21.6 |
| MiniGPT-4-v2                  |  23   |  31.1 |  32.9 |  13.2 |  17   |  25.7 |  22.2 |  26.3 |  24   |  10.8 |  16.2 |  23   |  20.6 |
| VisualGLM                     |  21.6 |  37.7 |  29.7 |  15.3 |  18.4 |  30.2 |  22.2 |  22.4 |  24   |   7.5 |   2.7 |  19   |  18.9 |
| InstructBLIP-13B              |  21.5 |  28.7 |  27.8 |  19.4 |  21.5 |  31.8 |  17.6 |  18.5 |  18.3 |  13.4 |  13.5 |  19   |  17.9 |
| IDEFICS-9B-Instruct           |  20.8 |  29.5 |  31   |  13.2 |  17.8 |  29.6 |  16.7 |  20.3 |  17.8 |   8.1 |  13.5 |  20.1 |  18.6 |
| MiniGPT-4-v1-13B              |  20.8 |  27   |  24.7 |   9   |  18.4 |  27.9 |  21.8 |  23.5 |  23.6 |   9.7 |  10.8 |  19.3 |  17.3 |
| MiniGPT-4-v1-7B               |  20.7 |  27.9 |  29.1 |   8.3 |  17.3 |  23.5 |  21.3 |  23.5 |  22.6 |  13.4 |   8.1 |  17.5 |  17.3 |
| OpenFlamingo v2               |  19   |  22.1 |  24.7 |   5.6 |  16.4 |  24   |  21.8 |  24.2 |  24   |   8.1 |  10.8 |  16   |  14.3 |
| **Random Chance**                 |  17.9 |  15.8 |  23.4 |   8.8 |  13.8 |  24.3 |  22.7 |  25.8 |  24.1 |   4.5 |  13.4 |  15.5 |  14.3 |
| Qwen-VL                       |  14.5 |  34.4 |  29.7 |  10.4 |  12.2 |  22.9 |   6.7 |   7.8 |   5.8 |   5.4 |  16.2 |  13   |  10.3 |